{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa5dfbe-1dea-43d5-ba2e-c2d306d617e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han leído 7769 documentos.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directorio de entrenamiento\n",
    "training_path = './reuters/training'  # Asegúrate de que esta ruta es correcta\n",
    "\n",
    "# Leer documentos\n",
    "documents = []\n",
    "for filename in os.listdir(training_path):\n",
    "    filepath = os.path.join(training_path, filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = f.read()\n",
    "        documents.append({'filename': filename, 'content': content})\n",
    "\n",
    "print(f\"Se han leído {len(documents)} documentos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b15f539-0feb-43b0-a4d5-e10e3f0cbc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpieza de datos completada.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Función para limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r'\\s+', ' ', texto)  # Eliminar espacios adicionales\n",
    "    texto = re.sub(r'\\W', ' ', texto)   # Eliminar caracteres no alfanuméricos\n",
    "    texto = texto.lower()               # Convertir a minúsculas\n",
    "    return texto\n",
    "\n",
    "# Limpiar documentos\n",
    "for doc in documents:\n",
    "    doc['clean_content'] = limpiar_texto(doc['content'])\n",
    "\n",
    "print(\"Limpieza de datos completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e39a4f1-10f5-479f-acda-bec9a419360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jair2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización completada.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar el paquete 'punkt' de NLTK si no está disponible\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenización\n",
    "for doc in documents:\n",
    "    doc['tokens'] = word_tokenize(doc['clean_content'])\n",
    "\n",
    "print(\"Tokenización completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1489668-e2f9-4198-a7a0-13278b33b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han leído 599 stopwords.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jair2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminación de stopwords y stemming completados.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Leer stopwords\n",
    "stopwords_path = './reuters/stopwords'  # Asegúrate de que esta ruta es correcta\n",
    "with open(stopwords_path, 'r') as f:\n",
    "    custom_stop_words = set(f.read().splitlines())\n",
    "\n",
    "# Descargar el paquete 'stopwords' de NLTK si no está disponible\n",
    "nltk.download('stopwords')\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "stop_words = custom_stop_words.union(nltk_stop_words)\n",
    "\n",
    "print(f\"Se han leído {len(stop_words)} stopwords.\")\n",
    "\n",
    "# Inicializar el stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Eliminar stopwords y aplicar stemming\n",
    "for doc in documents:\n",
    "    doc['tokens'] = [ps.stem(word) for word in doc['tokens'] if word not in stop_words]\n",
    "\n",
    "print(\"Eliminación de stopwords y stemming completados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742c41b3-e704-4d28-b97d-fddadad57fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento: 1\n",
      "Contenido original: BAHIA COCOA REVIEW\n",
      "  Showers continued throughout the week in\n",
      "  the Bahia cocoa zone, alleviating th...\n",
      "Tokens: ['bahia', 'cocoa', 'review', 'shower', 'continu', 'week', 'bahia', 'cocoa', 'zone', 'allevi']\n",
      "\n",
      "Documento: 10\n",
      "Contenido original: COMPUTER TERMINAL SYSTEMS &lt;CPML> COMPLETES SALE\n",
      "  Computer Terminal Systems Inc said\n",
      "  it has com...\n",
      "Tokens: ['comput', 'termin', 'system', 'lt', 'cpml', 'complet', 'sale', 'comput', 'termin', 'system']\n",
      "\n",
      "Documento: 100\n",
      "Contenido original: N.Z. TRADING BANK DEPOSIT GROWTH RISES SLIGHTLY\n",
      "  New Zealand's trading bank seasonally\n",
      "  adjusted d...\n",
      "Tokens: ['trade', 'bank', 'deposit', 'growth', 'rise', 'slightli', 'zealand', 'trade', 'bank', 'season']\n",
      "\n",
      "Documento: 1000\n",
      "Contenido original: NATIONAL AMUSEMENTS AGAIN UPS VIACOM &lt;VIA> BID\n",
      "  Viacom International Inc said &lt;National\n",
      "  Amu...\n",
      "Tokens: ['nation', 'amus', 'up', 'viacom', 'lt', 'bid', 'viacom', 'intern', 'lt', 'nation']\n",
      "\n",
      "Documento: 10000\n",
      "Contenido original: ROGERS &lt;ROG> SEES 1ST QTR NET UP SIGNIFICANTLY\n",
      "  Rogers Corp said first quarter\n",
      "  earnings will b...\n",
      "Tokens: ['roger', 'lt', 'rog', 'see', '1st', 'qtr', 'net', 'significantli', 'roger', 'corp']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Verificar contenido de los documentos\n",
    "\n",
    "\n",
    "# Verificar manualmente algunos archivos\n",
    "sample_files = documents[:5]  # Tomar los primeros 5 documentos para verificar\n",
    "for doc in sample_files:\n",
    "    print(f\"Documento: {doc['filename']}\")\n",
    "    print(f\"Contenido original: {doc['content'][:100]}...\")  # Mostrar primeros 100 caracteres\n",
    "    print(f\"Tokens: {doc['tokens'][:10]}\")  # Mostrar primeros 10 tokens\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b2a943-0d9a-4dba-8765-c41e4d6474c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz BoW de dimensiones: (7769, 19432)\n",
      "Matriz TF-IDF de dimensiones: (7769, 19432)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Convertir los tokens de los documentos a una representación textual\n",
    "clean_contents = [' '.join(doc['tokens']) for doc in documents]\n",
    "\n",
    "# Inicializar y ajustar el vectorizador Bag of Words\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform(clean_contents)\n",
    "\n",
    "# Inicializar y ajustar el vectorizador TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(clean_contents)\n",
    "\n",
    "print(f\"Matriz BoW de dimensiones: {X_bow.shape}\")\n",
    "print(f\"Matriz TF-IDF de dimensiones: {X_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c41c453a-2a5c-4fc1-93a3-180562c8e782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión usando BoW: 0.0000\n",
      "Precisión usando TF-IDF: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "cats_path = './reuters/cats.txt' \n",
    "# Suponiendo que las categorías están disponibles y son necesarias para las etiquetas\n",
    "# Si necesitas crear etiquetas desde las categorías, adapta este código\n",
    "labels = [doc['cats_path'].split('.')[0] for doc in documents]  # Esto es solo un ejemplo, ajusta según tu necesidad\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, labels, test_size=0.2, random_state=42)\n",
    "X_train_tfidf, X_test_tfidf = train_test_split(X_tfidf, test_size=0.2, random_state=42)[0:2]  # Mismas divisiones para ambos\n",
    "\n",
    "# Entrenar y evaluar usando BoW\n",
    "clf_bow = MultinomialNB()\n",
    "clf_bow.fit(X_train_bow, y_train)\n",
    "y_pred_bow = clf_bow.predict(X_test_bow)\n",
    "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
    "\n",
    "# Entrenar y evaluar usando TF-IDF\n",
    "clf_tfidf = MultinomialNB()\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "print(f\"Precisión usando BoW: {accuracy_bow:.4f}\")\n",
    "print(f\"Precisión usando TF-IDF: {accuracy_tfidf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a263c0-4519-4c3c-9508-3b66178a0c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef9148-3bc2-419c-b2b8-0430290a4997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0458e7-82e8-4e03-8793-bb029fc03922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17437b-526a-4ac9-8209-09f4a9aa2352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
