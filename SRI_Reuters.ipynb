{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650dc618-62d4-4218-a2dd-ead1652d5436",
   "metadata": {},
   "source": [
    "# Sistema de Recuperación de Información basado en Reuters-21578\r\n",
    "Integrantes: Cristina Molina, Jair Sanchez\r\n",
    "\r\n",
    "## Descripción del Proyecto\r\n",
    "\r\n",
    "Este proyecto se centra en el desarrollo de un Sistema de Recuperación de Información (SRI) utilizando el corpus Reuters-21578, un conjunto de datos ampliamente utilizado en la investigación de recuperación de información. El objetivo principal es implementar un sistema que permita realizar búsquedas eficientes y precisas dentro del corpus, utilizando técnicas modernas de procesamiento de texto y algoritmos de búsqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f29908-3b36-4c9e-9008-d2f6a56c59d1",
   "metadata": {},
   "source": [
    "## Bibliotecas y Herramientas Utilizadas\r\n",
    "\r\n",
    "Las siguientes bibliotecas y herramientas fueron utilizadas en este proyecto para realizar el preprocesamiento de texto y otras tareas relacionadas:\r\n",
    "\r\n",
    "- **Python 3:** Lenguaje de programación utilizado para desarrollar el código.\r\n",
    "\r\n",
    "- **NLTK (Natural Language Toolkit):** Biblioteca de Python ampliamente utilizada en el procesamiento del lenguaje natural. En particular, se utilizó para tokenización (`word_tokenize`) y stemming (`SnowballStemmer`).\r\n",
    "\r\n",
    "- **Scikit-learn:** Biblioteca de aprendizaje automático para Python que incluye módulos para vectorización de texto (`CountVectorizer`, `TfidfVectorizer`) y cálculo de similitud coseno (`cosine_similarity`).\r\n",
    "\r\n",
    "- **Pandas:** Biblioteca de Python utilizada para la manipulación y análisis de datos, en este caso, para cargar y manipular datos estructurados, como archivos CSV.\r\n",
    "\r\n",
    "- **CSV:** Módulo estándar de Python para la lectura y escritura de archivos CSV, utilizado para almacenar los resultados del preprocesamiento de documentos.\r\n",
    "\r\n",
    "- **Re (Regular Expressions):** Módulo de Python para trabajar con expresiones regulares. Se utilizó para filtrar y limpiar texto mediante patrones específicos.\r\n",
    "\r\n",
    "- **OS:** Módulo estándar de Python que proporciona funciones para interactuar con el sistema operativo, utilizado para manejar rutas de archivos y directorios.\r\n",
    "\r\n",
    "Estas bibliotecas y herramientas proporcionan las funcionalidades necesarias para realizar el preprocesamiento de texto, la vectorización de documentos y el cálculo de similitud, preparando así los datos para análisis adicionales en el campo del procesamiento del lenguaje natural y la recuperación de información.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e50b00-4ff4-45de-a69b-cd8c1330b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3342a-a5eb-4969-ad2e-6a5e7af1c112",
   "metadata": {},
   "source": [
    "# Fases del Proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad57ca-5003-4eb4-9c6f-ea523c4e65c8",
   "metadata": {},
   "source": [
    "## Fase 1: Adquisición de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a22593-2540-49a2-8631-fc7b369f9893",
   "metadata": {},
   "source": [
    " **Objetivo:**\r\n",
    "\r\n",
    "El objetivo de esta fase es obtener, descomprimir y organizar el corpus Reuters-21578 de manera que esté listo para ser preprocesado en las siguientes fases del proyecto.\r\n",
    "\r\n",
    "**Descripción**:\r\n",
    "\r\n",
    "El corpus Reuters-21578 es un conjunto de datos ampliamente utilizado en la investigación de recuperación de información y procesamiento de lenguaje natural. Contiene artículos de noticias clasificados en varias categorías, y está disponible públicamente para su uso en investigación y desarrollo.\r\n",
    "\r\n",
    "**Pasos para la Adquisición de Datos**\r\n",
    "1. **Descarga del Corpus Reuters-21578:** El primer paso es descargar el corpus desde una fuente confiable. El corpus está disponible en varios sitios de la web, pero se recomienda obtenerlo desde el sitio original de la Universidad de Carnegie Mellon (CMU).\r\n",
    "\r\n",
    "         URL de descarga: https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\r\n",
    "         \r\n",
    "  Pero para este proyecto se descargo la data directamente desde el repositorio proporcionado en el aula virtual\r\n",
    "\r\n",
    "2. **Descompresión y Organización de Archivos:** Una vez descargado el archivo comprimido, el siguiente paso es descomprimirlo y organizar los archivos en una estructura de directorios que facilite su acceso y manipulación.y manipulación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd00b30-e508-4296-81c2-7365b9519058",
   "metadata": {},
   "source": [
    "## Fase 2: Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab839d8-b677-4b56-aecb-768dfd6450e9",
   "metadata": {},
   "source": [
    "#### **Objetivo:**\n",
    "\n",
    "El objetivo de esta fase es preparar los documentos de texto para análisis posterior mediante la aplicación de varias técnicas de limpieza y transformación.\n",
    "\n",
    "#### **Descripción:**\n",
    "\n",
    "El preprocesamiento de texto es una etapa fundamental en el procesamiento del lenguaje natural (NLP). En este proyecto, se implementan cuatro fases para estructurar y limpiar los documentos de texto antes de su análisis y modelado.\n",
    "\n",
    "#### **Pasos del preprocesamiento:**\n",
    "\n",
    "1. **Convertir a minúsculas:**\n",
    "   - Todos los caracteres del texto se convierten a minúsculas para asegurar consistencia en el análisis, independientemente de las mayúsculas utilizadas en el texto original.\n",
    "\n",
    "2. **Eliminar caracteres no alfabéticos y dígitos:**\n",
    "   - Se utiliza la función `translate()` para eliminar caracteres no alfabéticos y dígitos del texto. Esto incluye signos de puntuación, símbolos y cualquier carácter que no sea una letra.\n",
    "\n",
    "3. **Eliminar stopwords:**\n",
    "   - Se eliminan las stopwords del texto. Las stopwords son palabras comunes pero no informativas que se filtran del texto para centrarse en las palabras que aportan un significado más relevante.\n",
    "\n",
    "4. **Aplicar stemming:**\n",
    "   - Cada palabra se reduce a su forma raíz utilizando el Snowball Stemmer en inglés. El stemming ayuda a normalizar las palabras al reducir sufijos y prefijos, lo que facilita la comparación y análisis posterior.\n",
    "\n",
    "Estas fases aseguran que los documentos de texto estén limpios y estructurados adecuadamente para su uso en tareas de análisis de texto y modelado en el campo del procesamiento del lenguaje natural.\n",
    "\n",
    "#### **Pasos adicionales:**\n",
    "\n",
    "- **Almacenamiento de Resultados:**\n",
    "  - Los documentos preprocesados se guardan en un archivo CSV llamado 'processed_documents.csv'. Este archivo contiene dos columnas: 'Filename' para el nombre del archivo original y 'Processed Text' para el texto procesado y limpio.\n",
    "\n",
    "Al finalizar este proceso, los documentos de texto están listos para ser utilizados en análisis posteriores, como la extracción de características, la comparación de similitud mediante modelos vectoriales, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071ed3b6-d257-4835-b627-f72c0ba3bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    # Abre el archivo en modo lectura\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        # Lee todas las líneas del archivo, elimina espacios en blanco alrededor de cada palabra y crea un conjunto de stopwords\n",
    "        stopwords = set(word.strip() for word in file.readlines())\n",
    "    # Retorna el conjunto de stopwords\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3478d09b-8403-449c-ac93-e6ea0f7185c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, stopwords):\n",
    "    # Convertir el texto a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Crear un traductor para eliminar signos de puntuación y dígitos\n",
    "    translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Tokenizar el texto en palabras\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Filtrar las palabras que no son stopwords\n",
    "    cleaned_tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    # Inicializar el stemmer para reducir las palabras a su raíz\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # Aplicar stemming a todas las palabras filtradas\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
    "    \n",
    "    # Unir las palabras procesadas en un solo texto limpio\n",
    "    cleaned_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    # Retornar el texto preprocesado y limpio\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6b412f-6285-4433-b95e-63709bb1134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo que contiene las stopwords\n",
    "stopwords_file = 'Proyecto_Data/reuters/stopwords.txt'\n",
    "\n",
    "# Cargar las stopwords desde el archivo\n",
    "stopwords = load_stopwords(stopwords_file)\n",
    "\n",
    "# Directorio donde se encuentran los archivos del corpus\n",
    "CORPUS_DIR = 'Proyecto_Data/reuters/training'\n",
    "\n",
    "# Diccionario para almacenar los textos limpios procesados\n",
    "diccionario = {}\n",
    "\n",
    "# Iterar sobre cada archivo en el directorio del corpus\n",
    "for filename in os.listdir(CORPUS_DIR):\n",
    "    # Construir la ruta completa al archivo\n",
    "    filepath = os.path.join(CORPUS_DIR, filename)\n",
    "    \n",
    "    # Abrir el archivo en modo lectura\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        # Leer todo el contenido del archivo\n",
    "        text = file.read()\n",
    "        \n",
    "        # Preprocesar el texto para limpiarlo y procesarlo\n",
    "        cleaned_text = preprocess_text(text, stopwords)\n",
    "        \n",
    "        # Almacenar el texto preprocesado en el diccionario, usando el nombre del archivo como clave\n",
    "        diccionario[filename] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f199da1-c7fb-4c32-9219-4af8189e27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre del archivo de salida donde se escribirán los documentos procesados\n",
    "output_file = 'processed_documents.csv'\n",
    "\n",
    "# Encabezados para las columnas del archivo CSV\n",
    "header = ['Filename', 'Processed Text']\n",
    "\n",
    "# Abrir el archivo CSV en modo escritura\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    # Crear un objeto escritor de CSV\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Escribir la primera fila con los encabezados\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Iterar sobre el diccionario que contiene los textos procesados\n",
    "    for filename, text in diccionario.items():\n",
    "        # Escribir cada par de nombre de archivo y texto procesado como una fila en el archivo CSV\n",
    "        writer.writerow([filename, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba277fa-597c-440d-b75b-8323b3ecf151",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'processed_documents.csv'\n",
    "df_textoPrepocesado = pd.read_csv(csv_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a53e38-a255-45d0-9e83-46d7c2123f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Processed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>bahia cocoa review shower continu week bahia c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.txt</td>\n",
       "      <td>comput termin system ltcpml complet sale compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.txt</td>\n",
       "      <td>nz trade bank deposit growth rise slight zeala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000.txt</td>\n",
       "      <td>nation amus up viacom ltvia bid viacom intern ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000.txt</td>\n",
       "      <td>roger ltrog see st qtr net signific roger corp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7764</th>\n",
       "      <td>999.txt</td>\n",
       "      <td>uk money market shortag forecast revis bank en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7765</th>\n",
       "      <td>9992.txt</td>\n",
       "      <td>knightridd ltkrn set quarter qtli div cts cts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7766</th>\n",
       "      <td>9993.txt</td>\n",
       "      <td>technitrol lttnl set quarter qtli div cts cts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7767</th>\n",
       "      <td>9994.txt</td>\n",
       "      <td>nationwid cellular servic ltncel qtr shr loss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7768</th>\n",
       "      <td>9995.txt</td>\n",
       "      <td>ltaha automot technolog corp year net shr cts ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7769 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Filename                                     Processed Text\n",
       "0         1.txt  bahia cocoa review shower continu week bahia c...\n",
       "1        10.txt  comput termin system ltcpml complet sale compu...\n",
       "2       100.txt  nz trade bank deposit growth rise slight zeala...\n",
       "3      1000.txt  nation amus up viacom ltvia bid viacom intern ...\n",
       "4     10000.txt  roger ltrog see st qtr net signific roger corp...\n",
       "...         ...                                                ...\n",
       "7764    999.txt  uk money market shortag forecast revis bank en...\n",
       "7765   9992.txt  knightridd ltkrn set quarter qtli div cts cts ...\n",
       "7766   9993.txt  technitrol lttnl set quarter qtli div cts cts ...\n",
       "7767   9994.txt  nationwid cellular servic ltncel qtr shr loss ...\n",
       "7768   9995.txt  ltaha automot technolog corp year net shr cts ...\n",
       "\n",
       "[7769 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_textoPrepocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ff497-08d8-4698-b9f2-c82a20101702",
   "metadata": {},
   "source": [
    "# Fase 3:  Representación de Datos en Espacio Vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07370dc-040b-43f7-9ce8-2352ef3a0f31",
   "metadata": {},
   "source": [
    "\r\n",
    "#### **Objetivo:**\r\n",
    "\r\n",
    "Esta fase tiene como objetivo transformar los documentos preprocesados en representaciones numéricas utilizando técnicas de vectorización, específicamente Bag of Words (BoW) y TF-IDF (Term Frequency-Inverse Document Frequency).\r\n",
    "\r\n",
    "#### **Descripción:**\r\n",
    "\r\n",
    "La representación de datos en espacio vectorial es fundamental para el procesamiento de lenguaje natural y la recuperación de información. En esta fase, se utilizan dos enfoques principales: BoW y TF-IDF, cada uno con sus propias características y aplicaciones.\r\n",
    "\r\n",
    "#### **Bag of Words (BoW):**\r\n",
    "\r\n",
    "El modelo Bag of Words (BoW) es una técnica de representación de documentos en la que se ignora el orden de las palabras y se considera solo su ocurrencia en el documento. Es útil para construir vectores de características que representan documentos de manera simple y efectiva.\r\n",
    "\r\n",
    "- **Fórmula:**\r\n",
    "  $$\r\n",
    "  \\text{BoW}(t, d) = \\text{count}(t, d)\r\n",
    "  $$\r\n",
    "  Donde:\r\n",
    "  - \\( t \\): Término (palabra).\r\n",
    "  - \\( d \\): Documento.\r\n",
    "  - \\( \\text{count}(t, d) \\): Frecuencia de aparición del término \\( t \\) en el documento \\( d \\).\r\n",
    "\r\n",
    "#### **TF-IDF (Term Frequency-Inverse Document Frequency):**\r\n",
    "\r\n",
    "TF-IDF es una medida estadística que evalúa la importancia de un término en un documento en el contexto de un conjunto de documentos (corpus). Combina la frecuencia de aparición de un término (TF) con la frecuencia inversa de documentos en los que aparece (IDF), permitiendo destacar términos que son frecuentes en un documento pero raros en el corpus general.\r\n",
    "\r\n",
    "- **Fórmula:**\r\n",
    "  $$\r\n",
    "  \\text{TF-IDF}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)\r\n",
    "  $$\r\n",
    "  Donde:\r\n",
    "  - \\( t \\): Término (palabra).\r\n",
    "  - \\( d \\): Documento.\r\n",
    "  - \\( \\text{tf}(t, d) \\): Frecuencia del término \\( t \\) en el documento \\( d \\).\r\n",
    "  - \\( \\text{idf}(t) \\): Inverso de la frecuencia de documentos que contienen el término \\( t \\) en el corpus.\r\n",
    "\r\n",
    "#### **Pasos para la representación:**\r\n",
    "\r\n",
    "1. **Bag of Words (BoW):**\r\n",
    "   - **Vectorización:** Se utiliza `CountVectorizer` de Scikit-learn para convertir cada documento preprocesado en un vector de términos, donde cada posición del vector representa la frecuencia de aparición de un término en el documento.\r\n",
    "   - **Almacenamiento:** Los vectores resultantes se almacenan en un DataFrame de Pandas (`df_bow`) con los nombres de los documentos como índices y los términos como columnas.\r\n",
    "   - **Exportación:** El DataFrame se guarda en un archivo CSV comprimido (`bow.csv.gz`) para facilitar el almacenamiento y la posterior carga.\r\n",
    "\r\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency):**\r\n",
    "   - **Vectorización:** Se utiliza `TfidfVectorizer` de Scikit-learn para calcular los pesos TF-IDF de cada término en cada documento. Este enfoque ajusta la importancia de los términos según su frecuencia en el documento y su frecuencia inversa en el corpus total.\r\n",
    "   - **Almacenamiento:** Los vectores TF-IDF se almacenan en otro DataFrame de Pandas (`df_tf_idf`), donde cada término tiene un peso calculado en función de su frecuencia en el documento y su raridad en el corpus general.\r\n",
    "   - **Exportación:** El DataFrame TF-IDF se guarda en un archivo CSV comprimido (`tf-idf.csv.gz`) para su posterior análisis y uso.\r\n",
    "\r\n",
    "Estos pasos aseguran que los documentos preprocesados estén representados de manera efectiva como vectores numéricos, preparándolos para técnicas avanzadas de análisis y modelado en el campo del procesamiento del lenguaje natural y la minería de textos.\r\n",
    "ento del lenguaje natural y la minería de textos.\r\n",
    "nto del lenguaje natural y la minería de textos.\r\n",
    "to del lenguaje natural y la minería de textos.\r\n",
    "ento del lenguaje natural y la minería de textos.\r\n",
    "to del lenguaje natural y la minería de textos.\r\n",
    "ento del lenguaje natural y la minería de textos.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e8bf2b-870d-41b9-8ef1-68b4f0a166b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la columna 'Processed Text' del DataFrame como una lista\n",
    "corpus = df_textoPrepocesado['Processed Text'].tolist()\n",
    "\n",
    "# Obtener la columna 'Filename' del DataFrame como una lista de nombres de textos\n",
    "nombres_textos = df_textoPrepocesado['Filename'].tolist()\n",
    "\n",
    "# Inicializar un vectorizador CountVectorizer con el parámetro binary=True\n",
    "vectorizerBoW = CountVectorizer(binary=True)\n",
    "\n",
    "# Aplicar el vectorizador al corpus para obtener la matriz de términos de documento (BoW)\n",
    "X = vectorizerBoW.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b23a932e-c019-4698-b502-617107adc460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aaminus</th>\n",
       "      <th>aancor</th>\n",
       "      <th>aap</th>\n",
       "      <th>aaplus</th>\n",
       "      <th>aar</th>\n",
       "      <th>aarnoud</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zorinski</th>\n",
       "      <th>zseven</th>\n",
       "      <th>zuccherifici</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulia</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurichbas</th>\n",
       "      <th>zuyuan</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7769 rows × 21411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           aa  aaa  aachen  aaminus  aancor  aap  aaplus  aar  aarnoud  aaron  \\\n",
       "1.txt       0    0       0        0       0    0       0    0        0      0   \n",
       "10.txt      0    0       0        0       0    0       0    0        0      0   \n",
       "100.txt     0    0       0        0       0    0       0    0        0      0   \n",
       "1000.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "10000.txt   0    0       0        0       0    0       0    0        0      0   \n",
       "...        ..  ...     ...      ...     ...  ...     ...  ...      ...    ...   \n",
       "999.txt     0    0       0        0       0    0       0    0        0      0   \n",
       "9992.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "9993.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "9994.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "9995.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "\n",
       "           ...  zorinski  zseven  zuccherifici  zuckerman  zulia  zurich  \\\n",
       "1.txt      ...         0       0             0          0      0       0   \n",
       "10.txt     ...         0       0             0          0      0       0   \n",
       "100.txt    ...         0       0             0          0      0       0   \n",
       "1000.txt   ...         0       0             0          0      0       0   \n",
       "10000.txt  ...         0       0             0          0      0       0   \n",
       "...        ...       ...     ...           ...        ...    ...     ...   \n",
       "999.txt    ...         0       0             0          0      0       0   \n",
       "9992.txt   ...         0       0             0          0      0       0   \n",
       "9993.txt   ...         0       0             0          0      0       0   \n",
       "9994.txt   ...         0       0             0          0      0       0   \n",
       "9995.txt   ...         0       0             0          0      0       0   \n",
       "\n",
       "           zurichbas  zuyuan  zverev  zzzz  \n",
       "1.txt              0       0       0     0  \n",
       "10.txt             0       0       0     0  \n",
       "100.txt            0       0       0     0  \n",
       "1000.txt           0       0       0     0  \n",
       "10000.txt          0       0       0     0  \n",
       "...              ...     ...     ...   ...  \n",
       "999.txt            0       0       0     0  \n",
       "9992.txt           0       0       0     0  \n",
       "9993.txt           0       0       0     0  \n",
       "9994.txt           0       0       0     0  \n",
       "9995.txt           0       0       0     0  \n",
       "\n",
       "[7769 rows x 21411 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizerBoW.get_feature_names_out(), index=nombres_textos)\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f7da50-99b2-431a-bb48-7eb4b49ace67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv_comprimido = 'bow.csv.gz'\n",
    "df_bow.to_csv(ruta_csv_comprimido, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa480f3-20ec-414e-b19e-4c00be6fc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los nombres de las características (palabras) del vectorizador BoW\n",
    "feature_names = vectorizerBoW.get_feature_names_out()\n",
    "\n",
    "# Lista para almacenar los vectores BoW de cada documento\n",
    "vectores_documentosBoW = []\n",
    "\n",
    "# Iterar sobre cada documento en el corpus\n",
    "for i, nombre_documento in enumerate(nombres_textos):\n",
    "    # Obtener el vector BoW del documento actual y convertirlo en un array plano\n",
    "    vector_documentoBoW = X[i].toarray().flatten()\n",
    "    \n",
    "    # Agregar el vector BoW a la lista de vectores de documentos BoW\n",
    "    vectores_documentosBoW.append(vector_documentoBoW)\n",
    "\n",
    "    # Ejemplo opcional de impresión (comentado)\n",
    "    # print(f\"Documento: {nombre_documento}\")\n",
    "    # print(f\"Vector: {vector_documentoBoW}\")\n",
    "    # print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19ea0da-8c5e-46c2-9fef-96c6b753572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar un vectorizador TF-IDF\n",
    "vectorizerTF_IDF = TfidfVectorizer()\n",
    "\n",
    "# Aplicar el vectorizador TF-IDF al corpus para obtener la matriz TF-IDF\n",
    "Y = vectorizerTF_IDF.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c14cad67-67d9-42b1-9693-5e01a338c97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aaminus</th>\n",
       "      <th>aancor</th>\n",
       "      <th>aap</th>\n",
       "      <th>aaplus</th>\n",
       "      <th>aar</th>\n",
       "      <th>aarnoud</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zorinski</th>\n",
       "      <th>zseven</th>\n",
       "      <th>zuccherifici</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulia</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurichbas</th>\n",
       "      <th>zuyuan</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7769 rows × 21411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            aa  aaa  aachen  aaminus  aancor  aap  aaplus  aar  aarnoud  \\\n",
       "1.txt      0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "10.txt     0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "100.txt    0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "1000.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "10000.txt  0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "...        ...  ...     ...      ...     ...  ...     ...  ...      ...   \n",
       "999.txt    0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9992.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9993.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9994.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9995.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "\n",
       "           aaron  ...  zorinski  zseven  zuccherifici  zuckerman  zulia  \\\n",
       "1.txt        0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "10.txt       0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "100.txt      0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "1000.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "10000.txt    0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "...          ...  ...       ...     ...           ...        ...    ...   \n",
       "999.txt      0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9992.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9993.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9994.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9995.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "\n",
       "           zurich  zurichbas  zuyuan  zverev  zzzz  \n",
       "1.txt         0.0        0.0     0.0     0.0   0.0  \n",
       "10.txt        0.0        0.0     0.0     0.0   0.0  \n",
       "100.txt       0.0        0.0     0.0     0.0   0.0  \n",
       "1000.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "10000.txt     0.0        0.0     0.0     0.0   0.0  \n",
       "...           ...        ...     ...     ...   ...  \n",
       "999.txt       0.0        0.0     0.0     0.0   0.0  \n",
       "9992.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "9993.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "9994.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "9995.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "\n",
       "[7769 rows x 21411 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf = pd.DataFrame(Y.toarray(), columns=vectorizerTF_IDF.get_feature_names_out(), index=nombres_textos)\n",
    "df_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4105044b-3db3-4a9c-8503-5da778233627",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv_comprimido = 'tf-idf.csv.gz'\n",
    "df_tf_idf.to_csv(ruta_csv_comprimido, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b97aca9-dec1-446c-b6ad-2c48abda0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los nombres de las características (palabras) del vectorizador TF-IDF\n",
    "feature_names = vectorizerTF_IDF.get_feature_names_out()\n",
    "\n",
    "# Lista para almacenar los vectores TF-IDF de cada documento\n",
    "vectores_documentosTF_IDF = []\n",
    "\n",
    "# Iterar sobre cada documento en el corpus\n",
    "for i, nombre_documento in enumerate(nombres_textos):\n",
    "    # Obtener el vector TF-IDF del documento actual y convertirlo en un array plano\n",
    "    vector_documentoTF_IDF = Y[i].toarray().flatten()\n",
    "    \n",
    "    # Agregar el vector TF-IDF a la lista de vectores de documentos TF-IDF\n",
    "    vectores_documentosTF_IDF.append(vector_documentoTF_IDF)\n",
    "\n",
    "    # Ejemplo opcional de impresión (comentado)\n",
    "    # print(f\"Documento: {nombre_documento}\")\n",
    "    # print(f\"Vector: {vector_documentoTF_IDF}\")\n",
    "    # print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efddbe78-16d1-435a-b211-75435d35a234",
   "metadata": {},
   "source": [
    "# Fase 4:  Indexación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42674f34-74fc-4e52-86d5-e80524f6de65",
   "metadata": {},
   "source": [
    "\r\n",
    "#### **Objetivo:**\r\n",
    "\r\n",
    "El objetivo de esta fase es construir un índice invertido a partir de un archivo de texto que contiene información sobre categorías asociadas a palabras clave encontradas en documentos preprocesados. Este índice invertido será utilizado para facilitar la recuperación eficiente de documentos relevantes durante la fase de búsqueda.\r\n",
    "\r\n",
    "#### **Descripción:**\r\n",
    "\r\n",
    "La indexación es una etapa crítica en los sistemas de recuperación de información (IR). En esta fase, se construye un índice invertido que mapea cada palabra clave a las categorías en las que aparece en los documentos procesados. Este índice facilita la búsqueda rápida y eficiente de documentos relevantes cuando se realiza una consulta de búsqueda.\r\n",
    "\r\n",
    "#### **Pasos para la construcción del índice invertido:**\r\n",
    "\r\n",
    "1. **Lectura del archivo de texto:**\r\n",
    "   - Se lee el archivo que contiene las categorías y las palabras clave asociadas a cada categoría.\r\n",
    "\r\n",
    "2. **Procesamiento de la información:**\r\n",
    "   - Cada línea del archivo se divide para extraer la categoría y las palabras clave asociadas.\r\n",
    "\r\n",
    "3. **Construcción del índice invertido:**\r\n",
    "   - Para cada palabra clave en la lista de palabras asociadas a una categoría, se añade la categoría al conjunto correspondiente en el índice invertido.\r\n",
    "   - Se utiliza un conjunto para almacenar las categorías asociadas a cada palabra clave, asegurando que no haya duplicados y optimizando la eficiencia de búsqueda.\r\n",
    "\r\n",
    "4. **Almacenamiento del índice invertido:**\r\n",
    "   - El índice invertido construido se guarda en un archivo de texto. Cada línea del archivo contiene una palabra clave seguida de las categorías en las que aparece*Resultados obtenidos:**\r\n",
    "\r\n",
    "Al finalizar esta fase, se obtiene un índice invertido completo que permite identificar rápidamente qué documentos contienen cada palabra clave consultada. Este índice será utilizado en la fase de búsqueda para recuperar documentos relentes de manera eficiente.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "363017db-514c-42be-b83e-4af904c95487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice invertido guardado en inverted_index.txt\n"
     ]
    }
   ],
   "source": [
    "def build_inverted_index(file_path):\n",
    "    inverted_index = {}\n",
    "    \n",
    "    # Abrir el archivo de texto\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Iterar sobre cada línea en el archivo\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            category = parts[0].split('/')[1]  # Obtener la categoría eliminando \"training/\"\n",
    "            words = parts[1:]\n",
    "\n",
    "            # Construir el índice invertido\n",
    "            for word in words:\n",
    "                if word not in inverted_index:\n",
    "                    inverted_index[word] = set()  # Usar un conjunto para evitar duplicados\n",
    "                inverted_index[word].add(category)\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "# Ruta del archivo de texto que contiene las categorías\n",
    "file_path = 'Proyecto_Data/reuters/cats.txt'\n",
    "\n",
    "# Construir el índice invertido\n",
    "inverted_index = build_inverted_index(file_path)\n",
    "\n",
    "# Escribir el índice invertido en un archivo de texto\n",
    "output_file = 'inverted_index.txt'\n",
    "with open(output_file, 'w') as out_file:\n",
    "    for word, categories in inverted_index.items():\n",
    "        out_file.write(f\"{word}: {', '.join(categories)}\\n\")\n",
    "\n",
    "print(\"Índice invertido guardado en\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db0457-cf0d-42a8-8051-f1bfedccc20c",
   "metadata": {},
   "source": [
    "# Fase 5:  Diseño del Motor de Búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86eeedd-5897-4c6d-9243-6f3b9d567155",
   "metadata": {},
   "source": [
    "#### **Objetivo:**\n",
    "\n",
    "El objetivo de esta fase es implementar un motor de búsqueda básico que pueda recuperar documentos relevantes basados en consultas del usuario. Utiliza técnicas de vectorización de consultas y medidas de similitud coseno sobre los vectores representativos de documentos para ordenar y presentar los resultados de manera efectiva.\n",
    "\n",
    "#### **Descripción:**\n",
    "\n",
    "El diseño del motor de búsqueda implica la utilización de técnicas de recuperación de información para identificar documentos que coincidan mejor con la consulta del usuario. En esta fase, se vectorizan tanto las consultas de los usuarios como los documentos almacenados previamente para calcular similitudes y así clasificar y presentar los documentos relevantes de manera ordenada.\n",
    "\n",
    "#### **Pasos clave del diseño del Motor de Búsqueda:**\n",
    "\n",
    "1. **Preprocesamiento de Consultas:**\n",
    "   - Las consultas de los usuarios se preprocesan utilizando técnicas como la conversión a minúsculas, eliminación de caracteres no alfabéticos, eliminación de stopwords y stemming. Esto asegura que la consulta esté en un formato limpio y consistente para el procesamiento posterior.\n",
    "\n",
    "2. **Vectorización de Consultas:**\n",
    "   - Las consultas preprocesadas se convierten en vectores utilizando técnicas de vectorización como Bag of Words (BoW) o TF-IDF (Term Frequency-Inverse Document Frequency). Estos vectores capturan la representación numérica de la consulta en función de la frecuencia de las palabras en la misma.\n",
    "\n",
    "3. **Cálculo de Similitud Coseno:**\n",
    "   - La similitud coseno es una medida que evalúa la similitud entre dos vectores en un espacio vectorial. Es particularmente útil en la recuperación de información para comparar la similitud entre la consulta del usuario y los documentos almacenados. La fórmula de similitud coseno entre dos vectores \\( A \\) y \\( B \\) se define como:\n",
    "$$\n",
    "   \\[\n",
    "   \\text{similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|}\n",
    "   \\]\n",
    "$$\n",
    "   Donde:\n",
    "   - \\( A \\cdot B \\) es el producto punto entre los vectores \\( A \\) y \\( B \\).\n",
    "   - \\( \\|A\\| \\) y \\( \\|B\\| \\) son las normas euclidianas de los vectores \\( A \\) y \\( B \\), respectivamente.\n",
    "\n",
    "\n",
    "   La similitud coseno devuelve un valor entre -1 y 1, donde 1 significa que los vectores son idénticos en dirección, 0 significa que son ortogonales (no tienen similitud), y -1 significa que son opuestos en dirección.\n",
    "\n",
    "4. **Ordenación de Resultados:**\n",
    "   - Los documentos se ordenan según su similitud coseno con respecto a la consulta, de mayor a menor similitud. Esto permite presentar los documentos más relevantes en la parte superior de los resultados de búsqueda.\n",
    "\n",
    "5. **Presentación de Resultados:**\n",
    "   - Los resultados ordenados se presentan al usuario, mostrando los nombres de los documentos junto con sus respectivas distancias (o similitudes) con respecto a la consulta. Esta presentación permite al usuario identificar rápidamente los documentos más relevantes a su consulta.\n",
    "\n",
    "Al finalizar esta fase, se obtiene un motor de búsqueda funcional capaz de recibir consultas de los usuarios, procesarlas, compararlas con documentos almacenados y presentar los documentos más relevantes de acuerdo con la similitud calculada. Este diseño forma la base para sistemas más avanzados de recuperación de información y búsqueda de información relevante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25849131-f2ae-4cf5-9e0d-586ae73f9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocesar_query(query):\n",
    "    query_prepocesada = preprocess_text(query, stopwords)\n",
    "    return query_prepocesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c181e6b-c55b-4f14-ad1c-59d33c729e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizar_consulta(query, vectorizer_type):\n",
    "    # Preprocesar la consulta\n",
    "    query_preprocesada = preprocess_text(query, stopwords)\n",
    "    \n",
    "    # Seleccionar y aplicar el vectorizador adecuado\n",
    "    if vectorizer_type == 'BoW':\n",
    "        vector_query = vectorizerBoW.transform([query_preprocesada])\n",
    "    elif vectorizer_type == 'TF-IDF':\n",
    "        vector_query = vectorizerTF_IDF.transform([query_preprocesada])\n",
    "    else:\n",
    "        raise ValueError(\"Tipo de vectorizador no válido. Use 'BoW' o 'TF-IDF'.\")\n",
    "    \n",
    "    return vector_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34706283-925c-4f4a-9433-d6ba85d425d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanciaBoW(query):\n",
    "    # Vectorizar la consulta utilizando Bag of Words (BoW)\n",
    "    vector_query = vectorizar_consulta(query, 'BoW')\n",
    "    \n",
    "    # Lista para almacenar las distancias coseno entre la consulta y cada documento\n",
    "    distancias = []\n",
    "    \n",
    "    # Calcular la distancia coseno entre la consulta y cada vector de documento BoW\n",
    "    for vector_documentoBoW in vectores_documentosBoW:\n",
    "        distancia = cosine_similarity(vector_query.reshape(1, -1), vector_documentoBoW.reshape(1, -1))[0][0]\n",
    "        distancias.append(distancia)\n",
    "    \n",
    "    return distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84f0182c-a84e-4eaa-8626-95261d9cc8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanciaTF_IDF(query):\n",
    "    # Vectorizar la consulta utilizando TF-IDF\n",
    "    vector_query = vectorizar_consulta(query, 'TF-IDF')\n",
    "    \n",
    "    # Lista para almacenar las distancias coseno entre la consulta y cada documento TF-IDF\n",
    "    distancias = []\n",
    "    \n",
    "    # Calcular la distancia coseno entre la consulta y cada vector de documento TF-IDF\n",
    "    for vector_documentoTF_IDF in vectores_documentosTF_IDF:\n",
    "        distancia = cosine_similarity(vector_query.reshape(1, -1), vector_documentoTF_IDF.reshape(1, -1))[0][0]\n",
    "        distancias.append(distancia)\n",
    "    \n",
    "    return distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e0fd812-4ee3-433f-882b-2f727e845b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_documentos(query, vectorizer_type):\n",
    "    if vectorizer_type == 'BoW':\n",
    "        distancias = distanciaBoW(query)\n",
    "    elif vectorizer_type == 'TF-IDF':\n",
    "        distancias = distanciaTF_IDF(query)\n",
    "    else:\n",
    "        raise ValueError(\"Tipo de vectorizador no válido. Use 'BoW' o 'TF-IDF'.\")\n",
    "\n",
    "    # Filtrar y ordenar simultáneamente usando una lista por comprensión con condición\n",
    "    resultados_ordenados = sorted(\n",
    "        ((nombre, distancia) for nombre, distancia in zip(nombres_textos, distancias) if distancia > 0),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Extraer solo los nombres ordenados\n",
    "    nombres_ordenados = [nombre for nombre, _ in resultados_ordenados]\n",
    "    \n",
    "    return resultados_ordenados, nombres_ordenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42563289-a552-4584-8c3a-3a334f84f849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busqueda para la query: lin-oil\n",
      "Resultados ordenados (nombre y distancia):\n",
      "Nombre: 6.txt, Distancia: 0.06864310990282788\n",
      "\n",
      "Nombres de documentos ordenados:\n",
      "6.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"lin-oil\"\n",
    "vectorizer_type = \"TF-IDF\"\n",
    "resultados, nombres_ordenados = buscar_documentos(query, vectorizer_type)\n",
    "\n",
    "print(\"Busqueda para la query: \" + query)\n",
    "print(\"Resultados ordenados (nombre y distancia):\")\n",
    "for nombre, distancia in resultados:\n",
    "    print(f\"Nombre: {nombre}, Distancia: {distancia}\")\n",
    "\n",
    "print(\"\\nNombres de documentos ordenados:\")\n",
    "for nombre in nombres_ordenados:\n",
    "    print(nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96db86c9-be47-406e-b23b-e9fc5e7de79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busqueda para la query: lin-oil\n",
      "Resultados ordenados (nombre y distancia):\n",
      "Nombre: 6.txt, Distancia: 0.15617376188860607\n",
      "\n",
      "Nombres de documentos ordenados:\n",
      "6.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"lin-oil\"\n",
    "vectorizer_type = \"BoW\"\n",
    "resultados, nombres_ordenados = buscar_documentos(query, vectorizer_type)\n",
    "\n",
    "print(\"Busqueda para la query: \" + query)\n",
    "print(\"Resultados ordenados (nombre y distancia):\")\n",
    "for nombre, distancia in resultados:\n",
    "    print(f\"Nombre: {nombre}, Distancia: {distancia}\")\n",
    "\n",
    "print(\"\\nNombres de documentos ordenados:\")\n",
    "for nombre in nombres_ordenados:\n",
    "    print(nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386ef32-41c8-44ee-812a-0d7d9396cc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
