{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650dc618-62d4-4218-a2dd-ead1652d5436",
   "metadata": {},
   "source": [
    "# Sistema de RecuperaciÃ³n de InformaciÃ³n basado en Reuters-21578\r\n",
    "Integrantes: Cristina Molina, Jair Sanchez\r\n",
    "\r\n",
    "## DescripciÃ³n del Proyecto\r\n",
    "\r\n",
    "Este proyecto se centra en el desarrollo de un Sistema de RecuperaciÃ³n de InformaciÃ³n (SRI) utilizando el corpus Reuters-21578, un conjunto de datos ampliamente utilizado en la investigaciÃ³n de recuperaciÃ³n de informaciÃ³n. El objetivo principal es implementar un sistema que permita realizar bÃºsquedas eficientes y precisas dentro del corpus, utilizando tÃ©cnicas modernas de procesamiento de texto y algoritmos de bÃºsqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f29908-3b36-4c9e-9008-d2f6a56c59d1",
   "metadata": {},
   "source": [
    "## Bibliotecas y Herramientas Utilizadas\r\n",
    "\r\n",
    "Las siguientes bibliotecas y herramientas fueron utilizadas en este proyecto para realizar el preprocesamiento de texto y otras tareas relacionadas:\r\n",
    "\r\n",
    "- **Python 3:** Lenguaje de programaciÃ³n utilizado para desarrollar el cÃ³digo.\r\n",
    "\r\n",
    "- **NLTK (Natural Language Toolkit):** Biblioteca de Python ampliamente utilizada en el procesamiento del lenguaje natural. En particular, se utilizÃ³ para tokenizaciÃ³n (`word_tokenize`) y stemming (`SnowballStemmer`).\r\n",
    "\r\n",
    "- **Scikit-learn:** Biblioteca de aprendizaje automÃ¡tico para Python que incluye mÃ³dulos para vectorizaciÃ³n de texto (`CountVectorizer`, `TfidfVectorizer`) y cÃ¡lculo de similitud coseno (`cosine_similarity`).\r\n",
    "\r\n",
    "- **Pandas:** Biblioteca de Python utilizada para la manipulaciÃ³n y anÃ¡lisis de datos, en este caso, para cargar y manipular datos estructurados, como archivos CSV.\r\n",
    "\r\n",
    "- **CSV:** MÃ³dulo estÃ¡ndar de Python para la lectura y escritura de archivos CSV, utilizado para almacenar los resultados del preprocesamiento de documentos.\r\n",
    "\r\n",
    "- **Re (Regular Expressions):** MÃ³dulo de Python para trabajar con expresiones regulares. Se utilizÃ³ para filtrar y limpiar texto mediante patrones especÃ­ficos.\r\n",
    "\r\n",
    "- **OS:** MÃ³dulo estÃ¡ndar de Python que proporciona funciones para interactuar con el sistema operativo, utilizado para manejar rutas de archivos y directorios.\r\n",
    "\r\n",
    "Estas bibliotecas y herramientas proporcionan las funcionalidades necesarias para realizar el preprocesamiento de texto, la vectorizaciÃ³n de documentos y el cÃ¡lculo de similitud, preparando asÃ­ los datos para anÃ¡lisis adicionales en el campo del procesamiento del lenguaje natural y la recuperaciÃ³n de informaciÃ³n.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e50b00-4ff4-45de-a69b-cd8c1330b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3342a-a5eb-4969-ad2e-6a5e7af1c112",
   "metadata": {},
   "source": [
    "# Fases del Proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad57ca-5003-4eb4-9c6f-ea523c4e65c8",
   "metadata": {},
   "source": [
    "## Fase 1: AdquisiciÃ³n de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a22593-2540-49a2-8631-fc7b369f9893",
   "metadata": {},
   "source": [
    " **Objetivo:**\r\n",
    "\r\n",
    "El objetivo de esta fase es obtener, descomprimir y organizar el corpus Reuters-21578 de manera que estÃ© listo para ser preprocesado en las siguientes fases del proyecto.\r\n",
    "\r\n",
    "**DescripciÃ³n**:\r\n",
    "\r\n",
    "El corpus Reuters-21578 es un conjunto de datos ampliamente utilizado en la investigaciÃ³n de recuperaciÃ³n de informaciÃ³n y procesamiento de lenguaje natural. Contiene artÃ­culos de noticias clasificados en varias categorÃ­as, y estÃ¡ disponible pÃºblicamente para su uso en investigaciÃ³n y desarrollo.\r\n",
    "\r\n",
    "**Pasos para la AdquisiciÃ³n de Datos**\r\n",
    "1. **Descarga del Corpus Reuters-21578:** El primer paso es descargar el corpus desde una fuente confiable. El corpus estÃ¡ disponible en varios sitios de la web, pero se recomienda obtenerlo desde el sitio original de la Universidad de Carnegie Mellon (CMU).\r\n",
    "\r\n",
    "         URL de descarga: https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\r\n",
    "         \r\n",
    "  Pero para este proyecto se descargo la data directamente desde el repositorio proporcionado en el aula virtual\r\n",
    "\r\n",
    "2. **DescompresiÃ³n y OrganizaciÃ³n de Archivos:** Una vez descargado el archivo comprimido, el siguiente paso es descomprimirlo y organizar los archivos en una estructura de directorios que facilite su acceso y manipulaciÃ³n.y manipulaciÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd00b30-e508-4296-81c2-7365b9519058",
   "metadata": {},
   "source": [
    "## Fase 2: Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab839d8-b677-4b56-aecb-768dfd6450e9",
   "metadata": {},
   "source": [
    "#### **Objetivo:**\n",
    "\n",
    "El objetivo de esta fase es preparar los documentos de texto para anÃ¡lisis posterior mediante la aplicaciÃ³n de varias tÃ©cnicas de limpieza y transformaciÃ³n.\n",
    "\n",
    "#### **DescripciÃ³n:**\n",
    "\n",
    "El preprocesamiento de texto es una etapa fundamental en el procesamiento del lenguaje natural (NLP). En este proyecto, se implementan cuatro fases para estructurar y limpiar los documentos de texto antes de su anÃ¡lisis y modelado.\n",
    "\n",
    "#### **Pasos del preprocesamiento:**\n",
    "\n",
    "1. **Convertir a minÃºsculas:**\n",
    "   - Todos los caracteres del texto se convierten a minÃºsculas para asegurar consistencia en el anÃ¡lisis, independientemente de las mayÃºsculas utilizadas en el texto original.\n",
    "\n",
    "2. **Eliminar caracteres no alfabÃ©ticos y dÃ­gitos:**\n",
    "   - Se utiliza la funciÃ³n `translate()` para eliminar caracteres no alfabÃ©ticos y dÃ­gitos del texto. Esto incluye signos de puntuaciÃ³n, sÃ­mbolos y cualquier carÃ¡cter que no sea una letra.\n",
    "\n",
    "3. **Eliminar stopwords:**\n",
    "   - Se eliminan las stopwords del texto. Las stopwords son palabras comunes pero no informativas que se filtran del texto para centrarse en las palabras que aportan un significado mÃ¡s relevante.\n",
    "\n",
    "4. **Aplicar stemming:**\n",
    "   - Cada palabra se reduce a su forma raÃ­z utilizando el Snowball Stemmer en inglÃ©s. El stemming ayuda a normalizar las palabras al reducir sufijos y prefijos, lo que facilita la comparaciÃ³n y anÃ¡lisis posterior.\n",
    "\n",
    "Estas fases aseguran que los documentos de texto estÃ©n limpios y estructurados adecuadamente para su uso en tareas de anÃ¡lisis de texto y modelado en el campo del procesamiento del lenguaje natural.\n",
    "\n",
    "#### **Pasos adicionales:**\n",
    "\n",
    "- **Almacenamiento de Resultados:**\n",
    "  - Los documentos preprocesados se guardan en un archivo CSV llamado 'processed_documents.csv'. Este archivo contiene dos columnas: 'Filename' para el nombre del archivo original y 'Processed Text' para el texto procesado y limpio.\n",
    "\n",
    "Al finalizar este proceso, los documentos de texto estÃ¡n listos para ser utilizados en anÃ¡lisis posteriores, como la extracciÃ³n de caracterÃ­sticas, la comparaciÃ³n de similitud mediante modelos vectoriales, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071ed3b6-d257-4835-b627-f72c0ba3bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    # Abre el archivo en modo lectura\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        # Lee todas las lÃ­neas del archivo, elimina espacios en blanco alrededor de cada palabra y crea un conjunto de stopwords\n",
    "        stopwords = set(word.strip() for word in file.readlines())\n",
    "    # Retorna el conjunto de stopwords\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3478d09b-8403-449c-ac93-e6ea0f7185c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, stopwords):\n",
    "    # Convertir el texto a minÃºsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Crear un traductor para eliminar signos de puntuaciÃ³n y dÃ­gitos\n",
    "    translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Tokenizar el texto en palabras\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Filtrar las palabras que no son stopwords\n",
    "    cleaned_tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    # Inicializar el stemmer para reducir las palabras a su raÃ­z\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # Aplicar stemming a todas las palabras filtradas\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
    "    \n",
    "    # Unir las palabras procesadas en un solo texto limpio\n",
    "    cleaned_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    # Retornar el texto preprocesado y limpio\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6b412f-6285-4433-b95e-63709bb1134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo que contiene las stopwords\n",
    "stopwords_file = 'Proyecto_Data/reuters/stopwords.txt'\n",
    "\n",
    "# Cargar las stopwords desde el archivo\n",
    "stopwords = load_stopwords(stopwords_file)\n",
    "\n",
    "# Directorio donde se encuentran los archivos del corpus\n",
    "CORPUS_DIR = 'Proyecto_Data/reuters/training'\n",
    "\n",
    "# Diccionario para almacenar los textos limpios procesados\n",
    "diccionario = {}\n",
    "\n",
    "# Iterar sobre cada archivo en el directorio del corpus\n",
    "for filename in os.listdir(CORPUS_DIR):\n",
    "    # Construir la ruta completa al archivo\n",
    "    filepath = os.path.join(CORPUS_DIR, filename)\n",
    "    \n",
    "    # Abrir el archivo en modo lectura\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        # Leer todo el contenido del archivo\n",
    "        text = file.read()\n",
    "        \n",
    "        # Preprocesar el texto para limpiarlo y procesarlo\n",
    "        cleaned_text = preprocess_text(text, stopwords)\n",
    "        \n",
    "        # Almacenar el texto preprocesado en el diccionario, usando el nombre del archivo como clave\n",
    "        diccionario[filename] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f199da1-c7fb-4c32-9219-4af8189e27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre del archivo de salida donde se escribirÃ¡n los documentos procesados\n",
    "output_file = 'processed_documents.csv'\n",
    "\n",
    "# Encabezados para las columnas del archivo CSV\n",
    "header = ['Filename', 'Processed Text']\n",
    "\n",
    "# Abrir el archivo CSV en modo escritura\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    # Crear un objeto escritor de CSV\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Escribir la primera fila con los encabezados\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Iterar sobre el diccionario que contiene los textos procesados\n",
    "    for filename, text in diccionario.items():\n",
    "        # Escribir cada par de nombre de archivo y texto procesado como una fila en el archivo CSV\n",
    "        writer.writerow([filename, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba277fa-597c-440d-b75b-8323b3ecf151",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'processed_documents.csv'\n",
    "df_textoPrepocesado = pd.read_csv(csv_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a53e38-a255-45d0-9e83-46d7c2123f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Processed Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>bahia cocoa review shower continu week bahia c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.txt</td>\n",
       "      <td>comput termin system ltcpml complet sale compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.txt</td>\n",
       "      <td>nz trade bank deposit growth rise slight zeala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000.txt</td>\n",
       "      <td>nation amus up viacom ltvia bid viacom intern ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000.txt</td>\n",
       "      <td>roger ltrog see st qtr net signific roger corp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7764</th>\n",
       "      <td>999.txt</td>\n",
       "      <td>uk money market shortag forecast revis bank en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7765</th>\n",
       "      <td>9992.txt</td>\n",
       "      <td>knightridd ltkrn set quarter qtli div cts cts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7766</th>\n",
       "      <td>9993.txt</td>\n",
       "      <td>technitrol lttnl set quarter qtli div cts cts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7767</th>\n",
       "      <td>9994.txt</td>\n",
       "      <td>nationwid cellular servic ltncel qtr shr loss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7768</th>\n",
       "      <td>9995.txt</td>\n",
       "      <td>ltaha automot technolog corp year net shr cts ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7769 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Filename                                     Processed Text\n",
       "0         1.txt  bahia cocoa review shower continu week bahia c...\n",
       "1        10.txt  comput termin system ltcpml complet sale compu...\n",
       "2       100.txt  nz trade bank deposit growth rise slight zeala...\n",
       "3      1000.txt  nation amus up viacom ltvia bid viacom intern ...\n",
       "4     10000.txt  roger ltrog see st qtr net signific roger corp...\n",
       "...         ...                                                ...\n",
       "7764    999.txt  uk money market shortag forecast revis bank en...\n",
       "7765   9992.txt  knightridd ltkrn set quarter qtli div cts cts ...\n",
       "7766   9993.txt  technitrol lttnl set quarter qtli div cts cts ...\n",
       "7767   9994.txt  nationwid cellular servic ltncel qtr shr loss ...\n",
       "7768   9995.txt  ltaha automot technolog corp year net shr cts ...\n",
       "\n",
       "[7769 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_textoPrepocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ff497-08d8-4698-b9f2-c82a20101702",
   "metadata": {},
   "source": [
    "# Fase 3:  RepresentaciÃ³n de Datos en Espacio Vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07370dc-040b-43f7-9ce8-2352ef3a0f31",
   "metadata": {},
   "source": [
    "\r\n",
    "#### **Objetivo:**\r\n",
    "\r\n",
    "Esta fase tiene como objetivo transformar los documentos preprocesados en representaciones numÃ©ricas utilizando tÃ©cnicas de vectorizaciÃ³n, especÃ­ficamente Bag of Words (BoW) y TF-IDF (Term Frequency-Inverse Document Frequency).\r\n",
    "\r\n",
    "#### **DescripciÃ³n:**\r\n",
    "\r\n",
    "La representaciÃ³n de datos en espacio vectorial es fundamental para el procesamiento de lenguaje natural y la recuperaciÃ³n de informaciÃ³n. En esta fase, se utilizan dos enfoques principales: BoW y TF-IDF, cada uno con sus propias caracterÃ­sticas y aplicaciones.\r\n",
    "\r\n",
    "#### **Bag of Words (BoW):**\r\n",
    "\r\n",
    "El modelo Bag of Words (BoW) es una tÃ©cnica de representaciÃ³n de documentos en la que se ignora el orden de las palabras y se considera solo su ocurrencia en el documento. Es Ãºtil para construir vectores de caracterÃ­sticas que representan documentos de manera simple y efectiva.\r\n",
    "\r\n",
    "- **FÃ³rmula:**\r\n",
    "  $$\r\n",
    "  \\text{BoW}(t, d) = \\text{count}(t, d)\r\n",
    "  $$\r\n",
    "  Donde:\r\n",
    "  - \\( t \\): TÃ©rmino (palabra).\r\n",
    "  - \\( d \\): Documento.\r\n",
    "  - \\( \\text{count}(t, d) \\): Frecuencia de apariciÃ³n del tÃ©rmino \\( t \\) en el documento \\( d \\).\r\n",
    "\r\n",
    "#### **TF-IDF (Term Frequency-Inverse Document Frequency):**\r\n",
    "\r\n",
    "TF-IDF es una medida estadÃ­stica que evalÃºa la importancia de un tÃ©rmino en un documento en el contexto de un conjunto de documentos (corpus). Combina la frecuencia de apariciÃ³n de un tÃ©rmino (TF) con la frecuencia inversa de documentos en los que aparece (IDF), permitiendo destacar tÃ©rminos que son frecuentes en un documento pero raros en el corpus general.\r\n",
    "\r\n",
    "- **FÃ³rmula:**\r\n",
    "  $$\r\n",
    "  \\text{TF-IDF}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)\r\n",
    "  $$\r\n",
    "  Donde:\r\n",
    "  - \\( t \\): TÃ©rmino (palabra).\r\n",
    "  - \\( d \\): Documento.\r\n",
    "  - \\( \\text{tf}(t, d) \\): Frecuencia del tÃ©rmino \\( t \\) en el documento \\( d \\).\r\n",
    "  - \\( \\text{idf}(t) \\): Inverso de la frecuencia de documentos que contienen el tÃ©rmino \\( t \\) en el corpus.\r\n",
    "\r\n",
    "#### **Pasos para la representaciÃ³n:**\r\n",
    "\r\n",
    "1. **Bag of Words (BoW):**\r\n",
    "   - **VectorizaciÃ³n:** Se utiliza `CountVectorizer` de Scikit-learn para convertir cada documento preprocesado en un vector de tÃ©rminos, donde cada posiciÃ³n del vector representa la frecuencia de apariciÃ³n de un tÃ©rmino en el documento.\r\n",
    "   - **Almacenamiento:** Los vectores resultantes se almacenan en un DataFrame de Pandas (`df_bow`) con los nombres de los documentos como Ã­ndices y los tÃ©rminos como columnas.\r\n",
    "   - **ExportaciÃ³n:** El DataFrame se guarda en un archivo CSV comprimido (`bow.csv.gz`) para facilitar el almacenamiento y la posterior carga.\r\n",
    "\r\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency):**\r\n",
    "   - **VectorizaciÃ³n:** Se utiliza `TfidfVectorizer` de Scikit-learn para calcular los pesos TF-IDF de cada tÃ©rmino en cada documento. Este enfoque ajusta la importancia de los tÃ©rminos segÃºn su frecuencia en el documento y su frecuencia inversa en el corpus total.\r\n",
    "   - **Almacenamiento:** Los vectores TF-IDF se almacenan en otro DataFrame de Pandas (`df_tf_idf`), donde cada tÃ©rmino tiene un peso calculado en funciÃ³n de su frecuencia en el documento y su raridad en el corpus general.\r\n",
    "   - **ExportaciÃ³n:** El DataFrame TF-IDF se guarda en un archivo CSV comprimido (`tf-idf.csv.gz`) para su posterior anÃ¡lisis y uso.\r\n",
    "\r\n",
    "Estos pasos aseguran que los documentos preprocesados estÃ©n representados de manera efectiva como vectores numÃ©ricos, preparÃ¡ndolos para tÃ©cnicas avanzadas de anÃ¡lisis y modelado en el campo del procesamiento del lenguaje natural y la minerÃ­a de textos.\r\n",
    "ento del lenguaje natural y la minerÃ­a de textos.\r\n",
    "nto del lenguaje natural y la minerÃ­a de textos.\r\n",
    "to del lenguaje natural y la minerÃ­a de textos.\r\n",
    "ento del lenguaje natural y la minerÃ­a de textos.\r\n",
    "to del lenguaje natural y la minerÃ­a de textos.\r\n",
    "ento del lenguaje natural y la minerÃ­a de textos.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e8bf2b-870d-41b9-8ef1-68b4f0a166b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la columna 'Processed Text' del DataFrame como una lista\n",
    "corpus = df_textoPrepocesado['Processed Text'].tolist()\n",
    "\n",
    "# Obtener la columna 'Filename' del DataFrame como una lista de nombres de textos\n",
    "nombres_textos = df_textoPrepocesado['Filename'].tolist()\n",
    "\n",
    "# Inicializar un vectorizador CountVectorizer con el parÃ¡metro binary=True\n",
    "vectorizerBoW = CountVectorizer(binary=True)\n",
    "\n",
    "# Aplicar el vectorizador al corpus para obtener la matriz de tÃ©rminos de documento (BoW)\n",
    "X = vectorizerBoW.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b23a932e-c019-4698-b502-617107adc460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aaminus</th>\n",
       "      <th>aancor</th>\n",
       "      <th>aap</th>\n",
       "      <th>aaplus</th>\n",
       "      <th>aar</th>\n",
       "      <th>aarnoud</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zorinski</th>\n",
       "      <th>zseven</th>\n",
       "      <th>zuccherifici</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulia</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurichbas</th>\n",
       "      <th>zuyuan</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7769 rows Ã— 21411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           aa  aaa  aachen  aaminus  aancor  aap  aaplus  aar  aarnoud  aaron  \\\n",
       "1.txt       0    0       0        0       0    0       0    0        0      0   \n",
       "10.txt      0    0       0        0       0    0       0    0        0      0   \n",
       "100.txt     0    0       0        0       0    0       0    0        0      0   \n",
       "1000.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "10000.txt   0    0       0        0       0    0       0    0        0      0   \n",
       "...        ..  ...     ...      ...     ...  ...     ...  ...      ...    ...   \n",
       "999.txt     0    0       0        0       0    0       0    0        0      0   \n",
       "9992.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "9993.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "9994.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "9995.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "\n",
       "           ...  zorinski  zseven  zuccherifici  zuckerman  zulia  zurich  \\\n",
       "1.txt      ...         0       0             0          0      0       0   \n",
       "10.txt     ...         0       0             0          0      0       0   \n",
       "100.txt    ...         0       0             0          0      0       0   \n",
       "1000.txt   ...         0       0             0          0      0       0   \n",
       "10000.txt  ...         0       0             0          0      0       0   \n",
       "...        ...       ...     ...           ...        ...    ...     ...   \n",
       "999.txt    ...         0       0             0          0      0       0   \n",
       "9992.txt   ...         0       0             0          0      0       0   \n",
       "9993.txt   ...         0       0             0          0      0       0   \n",
       "9994.txt   ...         0       0             0          0      0       0   \n",
       "9995.txt   ...         0       0             0          0      0       0   \n",
       "\n",
       "           zurichbas  zuyuan  zverev  zzzz  \n",
       "1.txt              0       0       0     0  \n",
       "10.txt             0       0       0     0  \n",
       "100.txt            0       0       0     0  \n",
       "1000.txt           0       0       0     0  \n",
       "10000.txt          0       0       0     0  \n",
       "...              ...     ...     ...   ...  \n",
       "999.txt            0       0       0     0  \n",
       "9992.txt           0       0       0     0  \n",
       "9993.txt           0       0       0     0  \n",
       "9994.txt           0       0       0     0  \n",
       "9995.txt           0       0       0     0  \n",
       "\n",
       "[7769 rows x 21411 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizerBoW.get_feature_names_out(), index=nombres_textos)\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f7da50-99b2-431a-bb48-7eb4b49ace67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv_comprimido = 'bow.csv.gz'\n",
    "df_bow.to_csv(ruta_csv_comprimido, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa480f3-20ec-414e-b19e-4c00be6fc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los nombres de las caracterÃ­sticas (palabras) del vectorizador BoW\n",
    "feature_names = vectorizerBoW.get_feature_names_out()\n",
    "\n",
    "# Lista para almacenar los vectores BoW de cada documento\n",
    "vectores_documentosBoW = []\n",
    "\n",
    "# Iterar sobre cada documento en el corpus\n",
    "for i, nombre_documento in enumerate(nombres_textos):\n",
    "    # Obtener el vector BoW del documento actual y convertirlo en un array plano\n",
    "    vector_documentoBoW = X[i].toarray().flatten()\n",
    "    \n",
    "    # Agregar el vector BoW a la lista de vectores de documentos BoW\n",
    "    vectores_documentosBoW.append(vector_documentoBoW)\n",
    "\n",
    "    # Ejemplo opcional de impresiÃ³n (comentado)\n",
    "    # print(f\"Documento: {nombre_documento}\")\n",
    "    # print(f\"Vector: {vector_documentoBoW}\")\n",
    "    # print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19ea0da-8c5e-46c2-9fef-96c6b753572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar un vectorizador TF-IDF\n",
    "vectorizerTF_IDF = TfidfVectorizer()\n",
    "\n",
    "# Aplicar el vectorizador TF-IDF al corpus para obtener la matriz TF-IDF\n",
    "Y = vectorizerTF_IDF.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c14cad67-67d9-42b1-9693-5e01a338c97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aaminus</th>\n",
       "      <th>aancor</th>\n",
       "      <th>aap</th>\n",
       "      <th>aaplus</th>\n",
       "      <th>aar</th>\n",
       "      <th>aarnoud</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zorinski</th>\n",
       "      <th>zseven</th>\n",
       "      <th>zuccherifici</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulia</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurichbas</th>\n",
       "      <th>zuyuan</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7769 rows Ã— 21411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            aa  aaa  aachen  aaminus  aancor  aap  aaplus  aar  aarnoud  \\\n",
       "1.txt      0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "10.txt     0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "100.txt    0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "1000.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "10000.txt  0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "...        ...  ...     ...      ...     ...  ...     ...  ...      ...   \n",
       "999.txt    0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9992.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9993.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9994.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9995.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "\n",
       "           aaron  ...  zorinski  zseven  zuccherifici  zuckerman  zulia  \\\n",
       "1.txt        0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "10.txt       0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "100.txt      0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "1000.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "10000.txt    0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "...          ...  ...       ...     ...           ...        ...    ...   \n",
       "999.txt      0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9992.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9993.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9994.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9995.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "\n",
       "           zurich  zurichbas  zuyuan  zverev  zzzz  \n",
       "1.txt         0.0        0.0     0.0     0.0   0.0  \n",
       "10.txt        0.0        0.0     0.0     0.0   0.0  \n",
       "100.txt       0.0        0.0     0.0     0.0   0.0  \n",
       "1000.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "10000.txt     0.0        0.0     0.0     0.0   0.0  \n",
       "...           ...        ...     ...     ...   ...  \n",
       "999.txt       0.0        0.0     0.0     0.0   0.0  \n",
       "9992.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "9993.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "9994.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "9995.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "\n",
       "[7769 rows x 21411 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf = pd.DataFrame(Y.toarray(), columns=vectorizerTF_IDF.get_feature_names_out(), index=nombres_textos)\n",
    "df_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4105044b-3db3-4a9c-8503-5da778233627",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv_comprimido = 'tf-idf.csv.gz'\n",
    "df_tf_idf.to_csv(ruta_csv_comprimido, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b97aca9-dec1-446c-b6ad-2c48abda0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los nombres de las caracterÃ­sticas (palabras) del vectorizador TF-IDF\n",
    "feature_names = vectorizerTF_IDF.get_feature_names_out()\n",
    "\n",
    "# Lista para almacenar los vectores TF-IDF de cada documento\n",
    "vectores_documentosTF_IDF = []\n",
    "\n",
    "# Iterar sobre cada documento en el corpus\n",
    "for i, nombre_documento in enumerate(nombres_textos):\n",
    "    # Obtener el vector TF-IDF del documento actual y convertirlo en un array plano\n",
    "    vector_documentoTF_IDF = Y[i].toarray().flatten()\n",
    "    \n",
    "    # Agregar el vector TF-IDF a la lista de vectores de documentos TF-IDF\n",
    "    vectores_documentosTF_IDF.append(vector_documentoTF_IDF)\n",
    "\n",
    "    # Ejemplo opcional de impresiÃ³n (comentado)\n",
    "    # print(f\"Documento: {nombre_documento}\")\n",
    "    # print(f\"Vector: {vector_documentoTF_IDF}\")\n",
    "    # print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efddbe78-16d1-435a-b211-75435d35a234",
   "metadata": {},
   "source": [
    "# Fase 4:  IndexaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42674f34-74fc-4e52-86d5-e80524f6de65",
   "metadata": {},
   "source": [
    "\r\n",
    "#### **Objetivo:**\r\n",
    "\r\n",
    "El objetivo de esta fase es construir un Ã­ndice invertido a partir de un archivo de texto que contiene informaciÃ³n sobre categorÃ­as asociadas a palabras clave encontradas en documentos preprocesados. Este Ã­ndice invertido serÃ¡ utilizado para facilitar la recuperaciÃ³n eficiente de documentos relevantes durante la fase de bÃºsqueda.\r\n",
    "\r\n",
    "#### **DescripciÃ³n:**\r\n",
    "\r\n",
    "La indexaciÃ³n es una etapa crÃ­tica en los sistemas de recuperaciÃ³n de informaciÃ³n (IR). En esta fase, se construye un Ã­ndice invertido que mapea cada palabra clave a las categorÃ­as en las que aparece en los documentos procesados. Este Ã­ndice facilita la bÃºsqueda rÃ¡pida y eficiente de documentos relevantes cuando se realiza una consulta de bÃºsqueda.\r\n",
    "\r\n",
    "#### **Pasos para la construcciÃ³n del Ã­ndice invertido:**\r\n",
    "\r\n",
    "1. **Lectura del archivo de texto:**\r\n",
    "   - Se lee el archivo que contiene las categorÃ­as y las palabras clave asociadas a cada categorÃ­a.\r\n",
    "\r\n",
    "2. **Procesamiento de la informaciÃ³n:**\r\n",
    "   - Cada lÃ­nea del archivo se divide para extraer la categorÃ­a y las palabras clave asociadas.\r\n",
    "\r\n",
    "3. **ConstrucciÃ³n del Ã­ndice invertido:**\r\n",
    "   - Para cada palabra clave en la lista de palabras asociadas a una categorÃ­a, se aÃ±ade la categorÃ­a al conjunto correspondiente en el Ã­ndice invertido.\r\n",
    "   - Se utiliza un conjunto para almacenar las categorÃ­as asociadas a cada palabra clave, asegurando que no haya duplicados y optimizando la eficiencia de bÃºsqueda.\r\n",
    "\r\n",
    "4. **Almacenamiento del Ã­ndice invertido:**\r\n",
    "   - El Ã­ndice invertido construido se guarda en un archivo de texto. Cada lÃ­nea del archivo contiene una palabra clave seguida de las categorÃ­as en las que aparece*Resultados obtenidos:**\r\n",
    "\r\n",
    "Al finalizar esta fase, se obtiene un Ã­ndice invertido completo que permite identificar rÃ¡pidamente quÃ© documentos contienen cada palabra clave consultada. Este Ã­ndice serÃ¡ utilizado en la fase de bÃºsqueda para recuperar documentos relentes de manera eficiente.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "363017db-514c-42be-b83e-4af904c95487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ãndice invertido guardado en inverted_index.txt\n"
     ]
    }
   ],
   "source": [
    "def build_inverted_index(file_path):\n",
    "    inverted_index = {}\n",
    "    \n",
    "    # Abrir el archivo de texto\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Iterar sobre cada lÃ­nea en el archivo\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            category = parts[0].split('/')[1]  # Obtener la categorÃ­a eliminando \"training/\"\n",
    "            words = parts[1:]\n",
    "\n",
    "            # Construir el Ã­ndice invertido\n",
    "            for word in words:\n",
    "                if word not in inverted_index:\n",
    "                    inverted_index[word] = set()  # Usar un conjunto para evitar duplicados\n",
    "                inverted_index[word].add(category)\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "# Ruta del archivo de texto que contiene las categorÃ­as\n",
    "file_path = 'Proyecto_Data/reuters/cats.txt'\n",
    "\n",
    "# Construir el Ã­ndice invertido\n",
    "inverted_index = build_inverted_index(file_path)\n",
    "\n",
    "# Escribir el Ã­ndice invertido en un archivo de texto\n",
    "output_file = 'inverted_index.txt'\n",
    "with open(output_file, 'w') as out_file:\n",
    "    for word, categories in inverted_index.items():\n",
    "        out_file.write(f\"{word}: {', '.join(categories)}\\n\")\n",
    "\n",
    "print(\"Ãndice invertido guardado en\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db0457-cf0d-42a8-8051-f1bfedccc20c",
   "metadata": {},
   "source": [
    "# Fase 5:  DiseÃ±o del Motor de BÃºsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86eeedd-5897-4c6d-9243-6f3b9d567155",
   "metadata": {},
   "source": [
    "#### **Objetivo:**\n",
    "\n",
    "El objetivo de esta fase es implementar un motor de bÃºsqueda bÃ¡sico que pueda recuperar documentos relevantes basados en consultas del usuario. Utiliza tÃ©cnicas de vectorizaciÃ³n de consultas y medidas de similitud coseno sobre los vectores representativos de documentos para ordenar y presentar los resultados de manera efectiva.\n",
    "\n",
    "#### **DescripciÃ³n:**\n",
    "\n",
    "El diseÃ±o del motor de bÃºsqueda implica la utilizaciÃ³n de tÃ©cnicas de recuperaciÃ³n de informaciÃ³n para identificar documentos que coincidan mejor con la consulta del usuario. En esta fase, se vectorizan tanto las consultas de los usuarios como los documentos almacenados previamente para calcular similitudes y asÃ­ clasificar y presentar los documentos relevantes de manera ordenada.\n",
    "\n",
    "#### **Pasos clave del diseÃ±o del Motor de BÃºsqueda:**\n",
    "\n",
    "1. **Preprocesamiento de Consultas:**\n",
    "   - Las consultas de los usuarios se preprocesan utilizando tÃ©cnicas como la conversiÃ³n a minÃºsculas, eliminaciÃ³n de caracteres no alfabÃ©ticos, eliminaciÃ³n de stopwords y stemming. Esto asegura que la consulta estÃ© en un formato limpio y consistente para el procesamiento posterior.\n",
    "\n",
    "2. **VectorizaciÃ³n de Consultas:**\n",
    "   - Las consultas preprocesadas se convierten en vectores utilizando tÃ©cnicas de vectorizaciÃ³n como Bag of Words (BoW) o TF-IDF (Term Frequency-Inverse Document Frequency). Estos vectores capturan la representaciÃ³n numÃ©rica de la consulta en funciÃ³n de la frecuencia de las palabras en la misma.\n",
    "\n",
    "3. **CÃ¡lculo de Similitud Coseno:**\n",
    "   - La similitud coseno es una medida que evalÃºa la similitud entre dos vectores en un espacio vectorial. Es particularmente Ãºtil en la recuperaciÃ³n de informaciÃ³n para comparar la similitud entre la consulta del usuario y los documentos almacenados. La fÃ³rmula de similitud coseno entre dos vectores \\( A \\) y \\( B \\) se define como:\n",
    "$$\n",
    "   \\[\n",
    "   \\text{similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|}\n",
    "   \\]\n",
    "$$\n",
    "   Donde:\n",
    "   - \\( A \\cdot B \\) es el producto punto entre los vectores \\( A \\) y \\( B \\).\n",
    "   - \\( \\|A\\| \\) y \\( \\|B\\| \\) son las normas euclidianas de los vectores \\( A \\) y \\( B \\), respectivamente.\n",
    "\n",
    "\n",
    "   La similitud coseno devuelve un valor entre -1 y 1, donde 1 significa que los vectores son idÃ©nticos en direcciÃ³n, 0 significa que son ortogonales (no tienen similitud), y -1 significa que son opuestos en direcciÃ³n.\n",
    "\n",
    "4. **OrdenaciÃ³n de Resultados:**\n",
    "   - Los documentos se ordenan segÃºn su similitud coseno con respecto a la consulta, de mayor a menor similitud. Esto permite presentar los documentos mÃ¡s relevantes en la parte superior de los resultados de bÃºsqueda.\n",
    "\n",
    "5. **PresentaciÃ³n de Resultados:**\n",
    "   - Los resultados ordenados se presentan al usuario, mostrando los nombres de los documentos junto con sus respectivas distancias (o similitudes) con respecto a la consulta. Esta presentaciÃ³n permite al usuario identificar rÃ¡pidamente los documentos mÃ¡s relevantes a su consulta.\n",
    "\n",
    "Al finalizar esta fase, se obtiene un motor de bÃºsqueda funcional capaz de recibir consultas de los usuarios, procesarlas, compararlas con documentos almacenados y presentar los documentos mÃ¡s relevantes de acuerdo con la similitud calculada. Este diseÃ±o forma la base para sistemas mÃ¡s avanzados de recuperaciÃ³n de informaciÃ³n y bÃºsqueda de informaciÃ³n relevante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25849131-f2ae-4cf5-9e0d-586ae73f9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocesar_query(query):\n",
    "    query_prepocesada = preprocess_text(query, stopwords)\n",
    "    return query_prepocesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c181e6b-c55b-4f14-ad1c-59d33c729e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizar_consulta(query, vectorizer_type):\n",
    "    # Preprocesar la consulta\n",
    "    query_preprocesada = preprocess_text(query, stopwords)\n",
    "    \n",
    "    # Seleccionar y aplicar el vectorizador adecuado\n",
    "    if vectorizer_type == 'BoW':\n",
    "        vector_query = vectorizerBoW.transform([query_preprocesada])\n",
    "    elif vectorizer_type == 'TF-IDF':\n",
    "        vector_query = vectorizerTF_IDF.transform([query_preprocesada])\n",
    "    else:\n",
    "        raise ValueError(\"Tipo de vectorizador no vÃ¡lido. Use 'BoW' o 'TF-IDF'.\")\n",
    "    \n",
    "    return vector_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34706283-925c-4f4a-9433-d6ba85d425d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanciaBoW(query):\n",
    "    # Vectorizar la consulta utilizando Bag of Words (BoW)\n",
    "    vector_query = vectorizar_consulta(query, 'BoW')\n",
    "    \n",
    "    # Lista para almacenar las distancias coseno entre la consulta y cada documento\n",
    "    distancias = []\n",
    "    \n",
    "    # Calcular la distancia coseno entre la consulta y cada vector de documento BoW\n",
    "    for vector_documentoBoW in vectores_documentosBoW:\n",
    "        distancia = cosine_similarity(vector_query.reshape(1, -1), vector_documentoBoW.reshape(1, -1))[0][0]\n",
    "        distancias.append(distancia)\n",
    "    \n",
    "    return distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84f0182c-a84e-4eaa-8626-95261d9cc8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanciaTF_IDF(query):\n",
    "    # Vectorizar la consulta utilizando TF-IDF\n",
    "    vector_query = vectorizar_consulta(query, 'TF-IDF')\n",
    "    \n",
    "    # Lista para almacenar las distancias coseno entre la consulta y cada documento TF-IDF\n",
    "    distancias = []\n",
    "    \n",
    "    # Calcular la distancia coseno entre la consulta y cada vector de documento TF-IDF\n",
    "    for vector_documentoTF_IDF in vectores_documentosTF_IDF:\n",
    "        distancia = cosine_similarity(vector_query.reshape(1, -1), vector_documentoTF_IDF.reshape(1, -1))[0][0]\n",
    "        distancias.append(distancia)\n",
    "    \n",
    "    return distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e0fd812-4ee3-433f-882b-2f727e845b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_documentos(query, vectorizer_type):\n",
    "    if vectorizer_type == 'BoW':\n",
    "        distancias = distanciaBoW(query)\n",
    "    elif vectorizer_type == 'TF-IDF':\n",
    "        distancias = distanciaTF_IDF(query)\n",
    "    else:\n",
    "        raise ValueError(\"Tipo de vectorizador no vÃ¡lido. Use 'BoW' o 'TF-IDF'.\")\n",
    "\n",
    "    # Filtrar y ordenar simultÃ¡neamente usando una lista por comprensiÃ³n con condiciÃ³n\n",
    "    resultados_ordenados = sorted(\n",
    "        ((nombre, distancia) for nombre, distancia in zip(nombres_textos, distancias) if distancia > 0),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Extraer solo los nombres ordenados\n",
    "    nombres_ordenados = [nombre for nombre, _ in resultados_ordenados]\n",
    "    \n",
    "    return resultados_ordenados, nombres_ordenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42563289-a552-4584-8c3a-3a334f84f849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busqueda para la query: lin-oil\n",
      "Resultados ordenados (nombre y distancia):\n",
      "Nombre: 6.txt, Distancia: 0.06864310990282788\n",
      "\n",
      "Nombres de documentos ordenados:\n",
      "6.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"lin-oil\"\n",
    "vectorizer_type = \"TF-IDF\"\n",
    "resultados, nombres_ordenados = buscar_documentos(query, vectorizer_type)\n",
    "\n",
    "print(\"Busqueda para la query: \" + query)\n",
    "print(\"Resultados ordenados (nombre y distancia):\")\n",
    "for nombre, distancia in resultados:\n",
    "    print(f\"Nombre: {nombre}, Distancia: {distancia}\")\n",
    "\n",
    "print(\"\\nNombres de documentos ordenados:\")\n",
    "for nombre in nombres_ordenados:\n",
    "    print(nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96db86c9-be47-406e-b23b-e9fc5e7de79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busqueda para la query: lin-oil\n",
      "Resultados ordenados (nombre y distancia):\n",
      "Nombre: 6.txt, Distancia: 0.15617376188860607\n",
      "\n",
      "Nombres de documentos ordenados:\n",
      "6.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"lin-oil\"\n",
    "vectorizer_type = \"BoW\"\n",
    "resultados, nombres_ordenados = buscar_documentos(query, vectorizer_type)\n",
    "\n",
    "print(\"Busqueda para la query: \" + query)\n",
    "print(\"Resultados ordenados (nombre y distancia):\")\n",
    "for nombre, distancia in resultados:\n",
    "    print(f\"Nombre: {nombre}, Distancia: {distancia}\")\n",
    "\n",
    "print(\"\\nNombres de documentos ordenados:\")\n",
    "for nombre in nombres_ordenados:\n",
    "    print(nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386ef32-41c8-44ee-812a-0d7d9396cc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
