{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cf6b8b-eebd-4d3f-ae65-2d18325bfd67",
   "metadata": {},
   "source": [
    "# Sistema de Recuperación de Información basado en Reuters-21578\r\n",
    "Integrantes: Cristina Molina, Jair Sanchez\r\n",
    "\r\n",
    "## Descripción del Proyecto\r\n",
    "\r\n",
    "Este proyecto se centra en el desarrollo de un Sistema de Recuperación de Información (SRI) utilizando el corpus Reuters-21578, un conjunto de datos ampliamente utilizado en la investigación de recuperación de información. El objetivo principal es implementar un sistema que permita realizar búsquedas eficientes y precisas dentro del corpus, utilizando técnicas modernas de procesamiento de texto y algoritmos de búsqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c38b0-38a2-496f-8fd3-2aaf8425d705",
   "metadata": {},
   "source": [
    "# Fases del Proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf7bda-b198-464b-836b-f4ac848dc6f6",
   "metadata": {},
   "source": [
    "## Fase 1: Adquisición de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3bdf4f-cef6-4dcd-a782-097ad69d5ae2",
   "metadata": {},
   "source": [
    " **Objetivo:**\n",
    "\n",
    "El objetivo de esta fase es obtener, descomprimir y organizar el corpus Reuters-21578 de manera que esté listo para ser preprocesado en las siguientes fases del proyecto.\n",
    "\n",
    "**Descripción**:\n",
    "\n",
    "El corpus Reuters-21578 es un conjunto de datos ampliamente utilizado en la investigación de recuperación de información y procesamiento de lenguaje natural. Contiene artículos de noticias clasificados en varias categorías, y está disponible públicamente para su uso en investigación y desarrollo.\n",
    "\n",
    "**Pasos para la Adquisición de Datos**\n",
    "1. **Descarga del Corpus Reuters-21578:** El primer paso es descargar el corpus desde una fuente confiable. El corpus está disponible en varios sitios de la web, pero se recomienda obtenerlo desde el sitio original de la Universidad de Carnegie Mellon (CMU).\n",
    "\n",
    "         URL de descarga: https://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html\n",
    "         \n",
    "  Pero para este proyecto se descargo la data directamente desde el repositorio proporcionado en el aula virtual\n",
    "\n",
    "2. **Descompresión y Organización de Archivos:** Una vez descargado el archivo comprimido, el siguiente paso es descomprimirlo y organizar los archivos en una estructura de directorios que facilite su acceso y manipulación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada02fe-fffc-4c42-a66c-ea7adc218154",
   "metadata": {},
   "source": [
    "## Fase 2: Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4365970b-21d2-4f52-9907-6384b6061411",
   "metadata": {},
   "source": [
    "#### **Objetivo:**\r\n",
    "\r\n",
    "El objetivo de este código es realizar el preprocesamiento de texto en archivos de texto plano ubicados en un directorio específico. Este preprocesamiento incluye la eliminación de palabras irrelevantes (stopwords), la conversión a minúsculas, la eliminación de caracteres no alfabéticos y números, la tokenización y el stemming de las palabras para reducirlas a su forma raíz.\r\n",
    "\r\n",
    "#### **Descripción:**\r\n",
    "\r\n",
    "El preprocesamiento de texto es una etapa fundamental en el procesamiento del lenguaje natural (NLP). Este proceso prepara el texto para su posterior análisis, eliminando ruido y normalizando el formato de las palabras. En este código, se utiliza para limpiar y estructurar el texto contenido en archivos de noticias de Reuters.\r\n",
    "\r\n",
    "#### **Pasos para el preprocesamiento:**\r\n",
    "\r\n",
    "1. **Descarga de Recursos Necesarios**\r\n",
    "    - Se descarga el recurso 'punkt' de la biblioteca NLTK, que se utiliza para la tokenización de palabras.\r\n",
    "\r\n",
    "2. **Carga de Stopwords**\r\n",
    "    - Se carga un conjunto de stopwords desde un archivo externo. Las stopwords son palabras comunes que no aportan un significado relevante al análisis y se eliminarán durante el prep*\r\n",
    "\r\n",
    "3. **Preprocesamiento de Texto:**\r\n",
    "    - **Convertir el texto a minúsculas:** \r\n",
    "      - Para garantizar la consistencia en el análisis, todo el texto se convierte a minúsculas. Esto es importante para que las palabras con diferencias de mayúsculas y minúsculas se traten de manera uniforme.\r\n",
    "    \r\n",
    "    - **Remover caracteres no alfabéticos y números:** \r\n",
    "      - Se utiliza una expresión regular para eliminar caracteres que no son letras del alfabeto y números. Esto incluye signos de puntuación, símbolos y números que no son relevantes para el análisis semántico d\n",
    "        \n",
    "      - La expresión regular utilizada es r'[^a-z\\s]'. Esta expresión regular busca coincidencias de cualquier carácter que no sea una letra minúscula del alfabeto inglés (a-z) ni un espacio en blanco (\\s). El modificador ^ dentro de los corchetes [^] indica negación, por lo que la expresión coincide con cualquier carácter que no sea una letra minúscula o un espacio en blanco. Esto permite eliminar signos de puntuación, símbolos y números del texto\n",
    "      xto.\r\n",
    "    \r\n",
    "    - **Tokenización:** \r\n",
    "      - El texto se divide en unidades significativas llamadas tokens utilizando la función `word_tokenize` de la biblioteca NLTK. Esta etapa es crucial para convertir el texto en una lista de palabras individuales que se pueden procesar y analizar por separado.\r\n",
    "    \r\n",
    "    - **Stemming:** \r\n",
    "      - Se aplica stemming a cada token utilizando el Snowball Stemmer en inglés. El stemming es el proceso de reducir cada palabra a su forma raíz, eliminando sufijos y prefijos para capturar la esencia de la palabra. Por ejemplo, \"running\" se reduce a \"run\" y \"walked\" se r\n",
    "        \n",
    "      - Se elige Snowball Stemmer en inglés por su eficacia y precisión en este idioma, conservando la integridad semántica de las palabras y siendo menos agresivo en la reducción. Su amplio uso y respaldo en la industria lo convierten en una elección confiable para el preprocesamiento de texto en inglés.\n",
    "         a \"walk\".\r\n",
    "    \r\n",
    "    - **Eliminación de stopwords:** \r\n",
    "      - Las stopwords, que son palabras comunes pero no informativas como \"el\", \"la\", \"de\", se eliminan del conjunto de tokens procesados. Esto se realiza comparando cada token con un conjunto predefinido de stopwords y eliminándo análisis de texto.de tokens procesados.\r\n",
    "\r\n",
    "4. **Guardado de los Resultados**\r\n",
    "    - Los documentos preprocesados se guardan en un archivo de texto llamado 'processed_documents.txt' en el mismo directorio que el código.\r\n",
    "\r\n",
    "Al finalizar el proceso, los documentos preprocesados están listos para ser utilizados en análisis adicionales, como la construcción de modelos de aprendizaje automático o la creación de índices invertidos para la recuperación de información.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96935923-010d-4006-b5ca-3beb362cdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Download the 'punkt' resource\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7aae7f5-6b83-4a37-b6d2-c936f388fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar stopwords desde un archivo\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "         stopwords = set(word.strip() for word in file.readlines())\n",
    "    return set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de97203-96ee-4e32-a0c9-2ec9c89a0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para preprocesar el texto\n",
    "def preprocess_text(text, stopwords):\n",
    "    # Convertir el texto a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remover caracteres no alfabéticos y números\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # Tokenizar el texto en palabras\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Inicializar el stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # Aplicar stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Eliminar stopwords después del stemming\n",
    "    cleaned_tokens = [token for token in stemmed_tokens if token not in stopwords]\n",
    "\n",
    "    # Unir los tokens limpios en una cadena de texto\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Ruta del archivo de stopwords\n",
    "stopwords_file = 'Proyecto_Data/reuters/stopwords.txt'\n",
    "\n",
    "# Cargar stopwords\n",
    "stopwords = load_stopwords(stopwords_file)\n",
    "\n",
    "# Directorio donde se encuentran los archivos\n",
    "CORPUS_DIR = 'Proyecto_Data/reuters/training'\n",
    "\n",
    "# Diccionario para almacenar los textos procesados de todos los archivos\n",
    "diccionario = {}\n",
    "\n",
    "# Procesar cada archivo en el directorio\n",
    "for filename in os.listdir(CORPUS_DIR):\n",
    "    filepath = os.path.join(CORPUS_DIR, filename)\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        text = file.read()\n",
    "        cleaned_text = preprocess_text(text, stopwords) \n",
    "        diccionario[filename] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0152d0-95be-4964-8bd9-149089032fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed documents saved to processed_documents.txt\n"
     ]
    }
   ],
   "source": [
    "# Guardar los resultados en un archivo\n",
    "output_file = 'processed_documents.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for filename, text in diccionario.items():\n",
    "        file.write(f\"{filename}: {text}\\n\")\n",
    "\n",
    "print(f\"Processed documents saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8f3f2-9c24-40ca-b8a3-0c2261df5b0b",
   "metadata": {},
   "source": [
    "# Fase 3:  Representación de Datos en Espacio Vectorial\n",
    "\n",
    "En esta fase, se lleva a cabo la representación de los datos textuales en un espacio vectorial para su posterior procesamiento y análisis. Se utilizan dos técnicas comunes: Bag of Words y TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdeed74-1c6e-42ed-8e0c-515e2a4b9f15",
   "metadata": {},
   "source": [
    "#### **Bag of Words (BoW)**\n",
    "\n",
    "El enfoque de Bag of Words es una técnica simple pero poderosa para representar datos de texto. Consiste en crear un vector que contiene la frecuencia de ocurrencia de cada palabra en un documento. Este enfoque ignora el orden de las palabras y solo considera su presencia o ausencia en el documento.\n",
    "\n",
    "- **Objetivo:**\n",
    "    - Convertir el corpus de documentos en una matriz donde cada fila representa un documento y cada columna representa una palabra, con el valor indicando la frecuencia de esa palabra en el documento.\n",
    "\n",
    "- **Descripción:**\n",
    "    - Se utiliza la clase `CountVectorizer` de la biblioteca scikit-learn para vectorizar el corpus utilizando Bag of Words.\n",
    "    - Los documentos se convierten en una lista de textos y luego se vectorizan usando `CountVectorizer`.\n",
    "    - El resultado es una matriz donde cada fila representa un documento y cada columna representa una palabra del vocabulario, con los valores indicando la frecuencia de esa palabra en el documento.\n",
    "\n",
    "- **Pasos:**\n",
    "    1. **Convertir el corpus a una lista de textos:** Se obtienen los textos de los documentos del diccionario.\n",
    "    2. **Vectorización usando Bag of Words:** Se utiliza `CountVectorizer` para transformar el corpus en una matriz de frecuencia de palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d905c570-501d-4df7-9ae9-413f6fd77014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convertir el corpus a una lista de textos\n",
    "corpus = list(diccionario.values())\n",
    "# Vectorización usando Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6db5f00c-cf46-4486-b030-016540d79009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aaminus</th>\n",
       "      <th>aancor</th>\n",
       "      <th>aap</th>\n",
       "      <th>aaplus</th>\n",
       "      <th>aar</th>\n",
       "      <th>aarnoud</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zorinski</th>\n",
       "      <th>zseven</th>\n",
       "      <th>zuccherifici</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulia</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurichbas</th>\n",
       "      <th>zuyuan</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7769 rows × 21406 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           aa  aaa  aachen  aaminus  aancor  aap  aaplus  aar  aarnoud  aaron  \\\n",
       "1.txt       0    0       0        0       0    0       0    0        0      0   \n",
       "10.txt      0    0       0        0       0    0       0    0        0      0   \n",
       "100.txt     0    0       0        0       0    0       0    0        0      0   \n",
       "1000.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "10000.txt   0    0       0        0       0    0       0    0        0      0   \n",
       "...        ..  ...     ...      ...     ...  ...     ...  ...      ...    ...   \n",
       "999.txt     0    0       0        0       0    0       0    0        0      0   \n",
       "9992.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "9993.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "9994.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "9995.txt    0    0       0        0       0    0       0    0        0      0   \n",
       "\n",
       "           ...  zorinski  zseven  zuccherifici  zuckerman  zulia  zurich  \\\n",
       "1.txt      ...         0       0             0          0      0       0   \n",
       "10.txt     ...         0       0             0          0      0       0   \n",
       "100.txt    ...         0       0             0          0      0       0   \n",
       "1000.txt   ...         0       0             0          0      0       0   \n",
       "10000.txt  ...         0       0             0          0      0       0   \n",
       "...        ...       ...     ...           ...        ...    ...     ...   \n",
       "999.txt    ...         0       0             0          0      0       0   \n",
       "9992.txt   ...         0       0             0          0      0       0   \n",
       "9993.txt   ...         0       0             0          0      0       0   \n",
       "9994.txt   ...         0       0             0          0      0       0   \n",
       "9995.txt   ...         0       0             0          0      0       0   \n",
       "\n",
       "           zurichbas  zuyuan  zverev  zzzz  \n",
       "1.txt              0       0       0     0  \n",
       "10.txt             0       0       0     0  \n",
       "100.txt            0       0       0     0  \n",
       "1000.txt           0       0       0     0  \n",
       "10000.txt          0       0       0     0  \n",
       "...              ...     ...     ...   ...  \n",
       "999.txt            0       0       0     0  \n",
       "9992.txt           0       0       0     0  \n",
       "9993.txt           0       0       0     0  \n",
       "9994.txt           0       0       0     0  \n",
       "9995.txt           0       0       0     0  \n",
       "\n",
       "[7769 rows x 21406 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out(), index=diccionario.keys())\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df2fdf-8446-4f21-bf45-1b4d38e81494",
   "metadata": {},
   "source": [
    "### **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "\n",
    "TF-IDF es otra técnica de representación de texto que tiene en cuenta tanto la frecuencia de ocurrencia de una palabra en un documento como la importancia de esa palabra en el conjunto de documentos. La importancia se basa en cuántas veces aparece una palabra en un documento (frecuencia de término) y en cuántos documentos contienen esa palabra (frecuencia inversa de documentos).\n",
    "\n",
    "- **Objetivo:**\n",
    "    - Convertir el corpus de documentos en una matriz donde cada fila representa un documento y cada columna representa una palabra, con el valor indicando la importancia de esa palabra en el documento mediante el esquema TF-IDF.\n",
    "\n",
    "- **Descripción:**\n",
    "    - Se utiliza la clase `TfidfVectorizer` de scikit-learn para vectorizar el corpus utilizando TF-IDF.\n",
    "    - Los documentos se convierten en una lista de textos y luego se vectorizan usando `TfidfVectorizer`.\n",
    "    - El resultado es una matriz donde cada fila representa un documento y cada columna representa una palabra del vocabulario, con los valores indicando la importancia de esa palabra en el documento mediante TF-IDF.\n",
    "\n",
    "- **Pasos:**\n",
    "    1. **Convertir el corpus a una lista de textos:** Se obtienen los textos de los documentos del diccionario.\n",
    "    2. **Vectorización usando TF-IDF:** Se utiliza `TfidfVectorizer` para transformar el corpus en una matriz TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96340a8a-29b1-478f-af3f-4bea9b9f84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convertir el corpus a una lista de textos\n",
    "corpus = list(diccionario.values())\n",
    "\n",
    "# Vectorización usando TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "Y = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ba2cfe-628a-4d3f-9acc-c70ee246be24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aachen</th>\n",
       "      <th>aaminus</th>\n",
       "      <th>aancor</th>\n",
       "      <th>aap</th>\n",
       "      <th>aaplus</th>\n",
       "      <th>aar</th>\n",
       "      <th>aarnoud</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zorinski</th>\n",
       "      <th>zseven</th>\n",
       "      <th>zuccherifici</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulia</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurichbas</th>\n",
       "      <th>zuyuan</th>\n",
       "      <th>zverev</th>\n",
       "      <th>zzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7769 rows × 21406 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            aa  aaa  aachen  aaminus  aancor  aap  aaplus  aar  aarnoud  \\\n",
       "1.txt      0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "10.txt     0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "100.txt    0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "1000.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "10000.txt  0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "...        ...  ...     ...      ...     ...  ...     ...  ...      ...   \n",
       "999.txt    0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9992.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9993.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9994.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "9995.txt   0.0  0.0     0.0      0.0     0.0  0.0     0.0  0.0      0.0   \n",
       "\n",
       "           aaron  ...  zorinski  zseven  zuccherifici  zuckerman  zulia  \\\n",
       "1.txt        0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "10.txt       0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "100.txt      0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "1000.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "10000.txt    0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "...          ...  ...       ...     ...           ...        ...    ...   \n",
       "999.txt      0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9992.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9993.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9994.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "9995.txt     0.0  ...       0.0     0.0           0.0        0.0    0.0   \n",
       "\n",
       "           zurich  zurichbas  zuyuan  zverev  zzzz  \n",
       "1.txt         0.0        0.0     0.0     0.0   0.0  \n",
       "10.txt        0.0        0.0     0.0     0.0   0.0  \n",
       "100.txt       0.0        0.0     0.0     0.0   0.0  \n",
       "1000.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "10000.txt     0.0        0.0     0.0     0.0   0.0  \n",
       "...           ...        ...     ...     ...   ...  \n",
       "999.txt       0.0        0.0     0.0     0.0   0.0  \n",
       "9992.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "9993.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "9994.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "9995.txt      0.0        0.0     0.0     0.0   0.0  \n",
       "\n",
       "[7769 rows x 21406 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf = pd.DataFrame(Y.toarray(), columns=vectorizer.get_feature_names_out(), index=diccionario.keys())\n",
    "df_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118dbbb4-c90c-4095-9cfa-a6a562459283",
   "metadata": {},
   "source": [
    "# Fase 4:  Indexación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f7607-6905-498c-9099-162f2006a90a",
   "metadata": {},
   "source": [
    "### Objetivo:\r\n",
    "La fase de indexación tiene como objetivo crear y guardar un índice invertido a partir de los datos vectorizados en forma de DataFrames. Este índice invertido permite recuperar rápidamente los documentos que contienen un término específico y proporciona información sobre la frecuencia de ese término en cada documento.\r\n",
    "\r\n",
    "### Descripción:\r\n",
    "La indexación es una etapa crucial en el procesamiento de datos textuales. En esta fase, se construye un índice invertido que mapea cada término del vocabulario a la lista de documentos en los que aparece, junto con la frecuencia de ese término en cada documento. Este índice es fundamental para la recuperación eficiente de información y para la realización de consultas en grandes conjuntos de datos textuales.\r\n",
    "\r\n",
    "### Pasos:\r\n",
    "1. **Crear el Índice Invertido:**\r\n",
    "    - Se define la función `crear_indice()` que recorre el DataFrame de términos y frecuencias para cada documento y construye un diccionario donde cada término del vocabulario se mapea a una lista de tuplas que contienen el identificador del documento y la frecuencia del término en ese documento.\r\n",
    "  \r\n",
    "2. **Guardar el Índice en Archivos de Texto:**\r\n",
    "    - Se define la función `guardar_indice()` que guarda el índice invertido en archivos de texto. Para cada término, se escribe su identificador de documento y su frecuencia asociada en el archivo.\r\n",
    "  spondientes.\r\n",
    "  \r\n",
    "4. **Crear y Guardar Índices para Bag of Words y TF-IDF:**\r\n",
    "    - Se crean los índices invertidos para los modelos Bag of Words (BoW) y TF-IDF, utilizando las funciones definidas anteriormente.\r\n",
    "  \r\n",
    "5. **Visualizar los Índices Creados:**\r\n",
    "    - Se llama a la función `visualizar_indice()` para mostrar los índices invertidos como DataFrames de Pandas, lo que permite inspeccionar fácilmente la estructura y el contenido del índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8aa5093-bea6-45f7-8b94-d668bdf8b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_indice(df):\n",
    "  indice_invertido = {}\n",
    "\n",
    "  for columna in df.columns:\n",
    "    for index, value in df[columna].items():\n",
    "      if value != 0:\n",
    "        if columna not in indice_invertido:\n",
    "          indice_invertido[columna] = []\n",
    "        indice_invertido[columna].append((index, value))\n",
    "\n",
    "  return indice_invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16387e7e-2e44-4932-90c0-0695c87c81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_indice(indice_invertido, directory, filename):\n",
    "  if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "  filepath = os.path.join(directory, filename)\n",
    "\n",
    "  with open(filepath, 'w') as file:\n",
    "    for termino, documentos in indice_invertido.items():\n",
    "      file.write(f\"Termino: {termino}\\n\")\n",
    "      for documento, frecuencia in documentos:\n",
    "        file.write(f\"Documento: {documento}, Frecuencia: {frecuencia}\\n\")\n",
    "      file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7036454-cb6d-4a98-9361-24f0cfc74a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear índices para Bag of Words y TF-IDF\n",
    "indice_bow = crear_indice(df_bow)\n",
    "indice_tf_idf = crear_indice(df_tf_idf)\n",
    "\n",
    "# Guardar índices a archivos de texto\n",
    "guardar_indice(indice_bow, 'Proyecto_Data/results', 'indice_bow.txt')\n",
    "guardar_indice(indice_tf_idf, 'Proyecto_Data/results', 'indice_tf_idf.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d3ce24-5da7-48cc-a5a1-f4b81388cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_indice(indice):\n",
    "    datos = []\n",
    "    for termino, docs in indice.items():\n",
    "        datos.append({'Termino': termino, 'Documentos': docs})\n",
    "    return pd.DataFrame(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e09fbb38-3252-44ca-936d-40a0ac72fda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Termino</th>\n",
       "      <th>Documentos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>[(12426.txt, 1), (3593.txt, 1), (5554.txt, 2)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaa</td>\n",
       "      <td>[(10931.txt, 1), (5554.txt, 2)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aachen</td>\n",
       "      <td>[(10648.txt, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaminus</td>\n",
       "      <td>[(10931.txt, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aancor</td>\n",
       "      <td>[(2133.txt, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21401</th>\n",
       "      <td>zurich</td>\n",
       "      <td>[(1159.txt, 1), (12878.txt, 1), (13005.txt, 1)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21402</th>\n",
       "      <td>zurichbas</td>\n",
       "      <td>[(302.txt, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21403</th>\n",
       "      <td>zuyuan</td>\n",
       "      <td>[(11768.txt, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21404</th>\n",
       "      <td>zverev</td>\n",
       "      <td>[(8856.txt, 2)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21405</th>\n",
       "      <td>zzzz</td>\n",
       "      <td>[(5738.txt, 1)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21406 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Termino                                         Documentos\n",
       "0             aa     [(12426.txt, 1), (3593.txt, 1), (5554.txt, 2)]\n",
       "1            aaa                    [(10931.txt, 1), (5554.txt, 2)]\n",
       "2         aachen                                   [(10648.txt, 1)]\n",
       "3        aaminus                                   [(10931.txt, 1)]\n",
       "4         aancor                                    [(2133.txt, 1)]\n",
       "...          ...                                                ...\n",
       "21401     zurich  [(1159.txt, 1), (12878.txt, 1), (13005.txt, 1)...\n",
       "21402  zurichbas                                     [(302.txt, 1)]\n",
       "21403     zuyuan                                   [(11768.txt, 1)]\n",
       "21404     zverev                                    [(8856.txt, 2)]\n",
       "21405       zzzz                                    [(5738.txt, 1)]\n",
       "\n",
       "[21406 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizar_indice(indice_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd906fa4-ba5c-4306-8f61-749541aff14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Termino</th>\n",
       "      <th>Documentos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>[(12426.txt, 0.05156352826941559), (3593.txt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaa</td>\n",
       "      <td>[(10931.txt, 0.041677586609165304), (5554.txt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aachen</td>\n",
       "      <td>[(10648.txt, 0.06672207061484473)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaminus</td>\n",
       "      <td>[(10931.txt, 0.04358502752710239)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aancor</td>\n",
       "      <td>[(2133.txt, 0.33629456014704673)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21401</th>\n",
       "      <td>zurich</td>\n",
       "      <td>[(1159.txt, 0.09215801585342415), (12878.txt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21402</th>\n",
       "      <td>zurichbas</td>\n",
       "      <td>[(302.txt, 0.05268770021537262)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21403</th>\n",
       "      <td>zuyuan</td>\n",
       "      <td>[(11768.txt, 0.047985451248573695)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21404</th>\n",
       "      <td>zverev</td>\n",
       "      <td>[(8856.txt, 0.198965200610787)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21405</th>\n",
       "      <td>zzzz</td>\n",
       "      <td>[(5738.txt, 0.3418853422487253)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21406 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Termino                                         Documentos\n",
       "0             aa  [(12426.txt, 0.05156352826941559), (3593.txt, ...\n",
       "1            aaa  [(10931.txt, 0.041677586609165304), (5554.txt,...\n",
       "2         aachen                 [(10648.txt, 0.06672207061484473)]\n",
       "3        aaminus                 [(10931.txt, 0.04358502752710239)]\n",
       "4         aancor                  [(2133.txt, 0.33629456014704673)]\n",
       "...          ...                                                ...\n",
       "21401     zurich  [(1159.txt, 0.09215801585342415), (12878.txt, ...\n",
       "21402  zurichbas                   [(302.txt, 0.05268770021537262)]\n",
       "21403     zuyuan                [(11768.txt, 0.047985451248573695)]\n",
       "21404     zverev                    [(8856.txt, 0.198965200610787)]\n",
       "21405       zzzz                   [(5738.txt, 0.3418853422487253)]\n",
       "\n",
       "[21406 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizar_indice(indice_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5cb016-da91-497f-8a81-fddcf81f3c47",
   "metadata": {},
   "source": [
    "# Fase 5:  Diseño del Motor de Búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725abbb8-293f-455b-9176-6cb20084c729",
   "metadata": {},
   "source": [
    "### Objetivo:\n",
    "La fase del diseño del Motor de Búsqueda se enfoca en la implementación de algoritmos de búsqueda para recuperar documentos relevantes basados en consultas de usuario. Se busca aplicar técnicas de similitud para calcular la relevancia de los documentos en función de las consultas ingresadas por el usuario.\n",
    "\n",
    "### Descripción:\n",
    "En esta fase, se implementan algoritmos de búsqueda que permiten recuperar documentos relevantes basados en consultas de usuario. Se utilizan dos enfoques principales: búsqueda por similitud de Jaccard y búsqueda por similitud coseno. Estos algoritmos evalúan la similitud entre el texto de la consulta y los documentos almacenados en el corpus, utilizando diferentes métricas para calcular la relevancia de cada documento.\n",
    "\n",
    "### Pasos:\n",
    "1. **Carga del Índice Invertido:**\n",
    "    - Se define la función `separar_indice(filepath)` para cargar el índice invertido desde un archivo de texto. Esta función lee el archivo línea por línea, extrayendo la información de los términos y la frecuencia de cada documento.\n",
    "    \n",
    "2. **Preprocesamiento de Consulta:**\n",
    "    - Se define la función `preprocesar_query(query)` para preprocesar la consulta del usuario. Esta función aplica el mismo preprocesamiento que se utilizó para los documentos del corpus, incluyendo la eliminación de stopwords y la tokenización.\n",
    "    \n",
    "3. **Cálculo de Similitud de Jaccard:**\n",
    "    - Se implementa la función `busqueda_jaccard(query, inverted_index)` para realizar la búsqueda basada en la similitud de Jaccard. Esta función calcula la similitud entre la consulta y cada documento en el corpus utilizando el coeficiente de Jaccard, que mide la intersección entre los términos de la consulta y los términos del documento.\n",
    "    \n",
    "4. **Cálculo de Similitud Coseno:**\n",
    "    - Se implementa la función `busqueda_coseno(query, inverted_index)` para realizar la búsqueda basada en la similitud coseno. Esta función calcula la similitud entre la consulta y cada documento en el corpus utilizando el producto escalar y la norma de los vectores de términos, lo que permite comparar la dirección y la magnitud de los vectores.\n",
    "    \n",
    "5. **Selección de Resultados:**\n",
    "    - Se define la función `results(query, tv, tr)` para seleccionar los resultados de la búsqueda en función de los parámetros `tv` y `tr`, que indican el tipo de vectorización (BoW o TF-IDF) y el tipo de algoritmo de búsqueda (Jaccard o coseno). Esta función llama a las funciones de búsqueda correspondientes y devuelve los documentos más relevantes para la consulta.\n",
    "    \n",
    "6. **Ejemplo de Uso:**\n",
    "    - Se proporciona un ejemplo de cómo utilizar las funciones de búsqueda para recuperar documentos relevantes en función de una consulta ingresada por el usuario. Esto incluye la llamada a la función `results()` con la consulta, la configuración de vectorización y el tipo de algoritmo de búsqueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378362f1-38f1-4a11-b71f-a584d86a2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separar_indice(filepath):\n",
    "    try:\n",
    "        print(f\"Cargando índice invertido desde: {filepath}\")\n",
    "        inverted_index = {}\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            current_term = None\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"Termino:\"):\n",
    "                    current_term = line.split(\"Termino: \")[1]\n",
    "                    inverted_index[current_term] = []\n",
    "                elif line.startswith(\"Documento:\"):\n",
    "                    doc_info = line.split(\"Documento: \")[1]\n",
    "                    doc_name, weight = doc_info.split(\", Frecuencia: \")\n",
    "                    inverted_index[current_term].append((doc_name, float(weight)))\n",
    "        return inverted_index\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Archivo no encontrado en {filepath}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el índice invertido desde {filepath}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b50800ff-6831-4b7b-a56e-dc2bb08919e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocesar_query(query):\n",
    "    query_prepocesada = preprocess_text(query, stopwords) \n",
    "    print(f\"Consulta procesada: {query_prepocesada}\")\n",
    "    return query_prepocesada.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd31ab4b-b392-4772-b7da-ab90440f3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similaridad(query_tokens, document_tokens):\n",
    "    intersection = len(set(query_tokens) & set(document_tokens))\n",
    "    union = len(set(query_tokens) | set(document_tokens))\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f851726-0edc-44bd-8d5f-f8a98ccc3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def coseno_similaridad(query_vector, doc_vector):\n",
    "    dot_product = sum(query_vector[term] * doc_vector.get(term, 0) for term in query_vector)\n",
    "    query_norm = math.sqrt(sum(weight ** 2 for weight in query_vector.values()))\n",
    "    doc_norm = math.sqrt(sum(weight ** 2 for weight in doc_vector.values()))\n",
    "    if query_norm == 0 or doc_norm == 0:\n",
    "        return 0\n",
    "    return dot_product / (query_norm * doc_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0a24796-999c-4b36-a55c-47060724d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def busqueda_jaccard(query, inverted_index):\n",
    "    try:\n",
    "        CORPUS_DIR = os.path.join(os.getcwd(), 'Proyecto_Data', 'reuters', 'training')\n",
    "        documents = {}\n",
    "        for filename in os.listdir(CORPUS_DIR):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                filepath = os.path.join(CORPUS_DIR, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    cleaned_text = preprocess_text(text, stopwords) \n",
    "                    documents[filename] = cleaned_text\n",
    "        query_tokens = prepocesar_query(query)\n",
    "        document_tokens = {doc_id: documents[doc_id].split() for doc_id in documents}\n",
    "        results = []\n",
    "        for doc_id in document_tokens:\n",
    "            similarity = jaccard_similaridad(query_tokens, document_tokens[doc_id])\n",
    "            results.append((doc_id, similarity))\n",
    "        ranked_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        doc_ids = [doc_id for doc_id, _ in ranked_results]\n",
    "        return doc_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Error in search_jaccard: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2d152e6-8c0e-40df-80bf-15b9e4906034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def busqueda_coseno(query, inverted_index):\n",
    "    try:\n",
    "        query_tokens = prepocesar_query(query)\n",
    "        query_vector = {term: query_tokens.count(term) for term in query_tokens}\n",
    "        document_vectors = defaultdict(dict)\n",
    "        for term in query_tokens:\n",
    "            if term in inverted_index:\n",
    "                for doc_id, tfidf_score in inverted_index[term]:\n",
    "                    document_vectors[doc_id][term] = tfidf_score\n",
    "        scores = {}\n",
    "        for doc_id in document_vectors:\n",
    "            scores[doc_id] = coseno_similaridad(query_vector, document_vectors[doc_id])\n",
    "        ranked_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        doc_ids = [doc_id for doc_id, _ in ranked_results]\n",
    "        return doc_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Error in search_cosine: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a679deeb-b2d6-4e04-b564-8ca9375cfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(query, tv, tr):\n",
    "    try:\n",
    "        if tv == \"0\" and tr == \"1\":\n",
    "            inverted_index_loaded = separar_indice(os.path.join(os.getcwd(), 'Proyecto_Data', 'results', 'indice_tf_idf.txt'))\n",
    "            results = busqueda_coseno(query, inverted_index_loaded)\n",
    "        elif tv == \"1\" and tr == \"0\":\n",
    "            inverted_index_loaded = separar_indice(os.path.join(os.getcwd(), 'Proyecto_Data', 'results', 'indice_bow.txt'))\n",
    "            results = busqueda_jaccard(query, inverted_index_loaded)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid combination of tv and tr values. Only BoW with Jaccard and TF-IDF with Cosine are allowed.\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in results function with query '{query}', tv='{tv}', tr='{tr}': {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62805e03-8d78-450d-815d-c94af1c61f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando índice invertido desde: C:\\Users\\User\\Documents\\Universidad\\7 semestre\\Recuperación Info\\Proyecto 2.0\\ProyectoDefinitivo2\\Proyecto_Data\\results\\indice_tf_idf.txt\n",
      "Consulta procesada: coffe\n",
      "Resultados de la búsqueda: ['10014.txt', '10100.txt', '1026.txt', '10268.txt', '1030.txt', '10375.txt', '10406.txt', '10640.txt', '10682.txt', '10693.txt', '10752.txt', '1085.txt', '10876.txt', '10902.txt', '10959.txt', '11183.txt', '11224.txt', '11265.txt', '11341.txt', '11372.txt', '11462.txt', '11816.txt', '11866.txt', '11882.txt', '11949.txt', '12008.txt', '12011.txt', '1207.txt', '1212.txt', '12152.txt', '12208.txt', '12340.txt', '12355.txt', '12399.txt', '12424.txt', '12426.txt', '1246.txt', '12465.txt', '12490.txt', '12814.txt', '12843.txt', '1312.txt', '13170.txt', '13190.txt', '13201.txt', '13242.txt', '13269.txt', '13834.txt', '14418.txt', '14698.txt', '1579.txt', '1715.txt', '1723.txt', '1842.txt', '1880.txt', '1889.txt', '1910.txt', '1960.txt', '2115.txt', '232.txt', '235.txt', '2467.txt', '249.txt', '2521.txt', '2550.txt', '2553.txt', '2606.txt', '275.txt', '290.txt', '2954.txt', '3034.txt', '3040.txt', '3187.txt', '3310.txt', '3559.txt', '3955.txt', '402.txt', '4063.txt', '4071.txt', '4132.txt', '4147.txt', '42.txt', '4267.txt', '4349.txt', '4564.txt', '4603.txt', '4634.txt', '4744.txt', '4785.txt', '5002.txt', '5057.txt', '5134.txt', '5238.txt', '5258.txt', '5334.txt', '5471.txt', '5491.txt', '5570.txt', '562.txt', '5684.txt', '5692.txt', '6338.txt', '6595.txt', '6632.txt', '6758.txt', '6912.txt', '7104.txt', '7124.txt', '7143.txt', '7367.txt', '7406.txt', '75.txt', '754.txt', '7888.txt', '8105.txt', '8149.txt', '8193.txt', '8200.txt', '842.txt', '875.txt', '8903.txt', '8950.txt', '9153.txt', '9265.txt', '9654.txt', '9680.txt', '977.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ejemplo de llamada a la función results\n",
    "query = \"coffee\"\n",
    "tv = \"0\"  # Para TF-IDF con Coseno\n",
    "tr = \"1\"  # Para TF-IDF con Coseno\n",
    "\n",
    "try:\n",
    "    # Llamada a la función results\n",
    "    search_results = results(query, tv, tr)\n",
    "    print(\"Resultados de la búsqueda:\", search_results)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inesperado: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
